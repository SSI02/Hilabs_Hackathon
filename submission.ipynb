{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0626e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "# Hyperparameter optimization\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not available - install with: pip install optuna\")\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "TRAIN_DIR = BASE_DIR / 'pcms_hackathon_data' / 'train'\n",
    "TEST_DIR = BASE_DIR / 'pcms_hackathon_data' / 'test'\n",
    "OUTPUT_DIR = BASE_DIR / 'analysis_outputs'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b20339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading and preparing data...\n",
      "✓ Data loaded\n",
      "\n",
      "  Creating data overview visualizations...\n",
      "  ✓ Saved: analysis_outputs/01_data_overview.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Loading and preparing data...\")\n",
    "\n",
    "# Load data\n",
    "train_patient = pd.read_csv(TRAIN_DIR / 'patient.csv')\n",
    "train_risk = pd.read_csv(TRAIN_DIR / 'risk.csv')\n",
    "train_visit = pd.read_csv(TRAIN_DIR / 'visit.csv')\n",
    "train_diagnosis = pd.read_csv(TRAIN_DIR / 'diagnosis.csv')\n",
    "train_care = pd.read_csv(TRAIN_DIR / 'care.csv')\n",
    "\n",
    "test_patient = pd.read_csv(TEST_DIR / 'patient.csv')\n",
    "test_visit = pd.read_csv(TEST_DIR / 'visit.csv')\n",
    "test_diagnosis = pd.read_csv(TEST_DIR / 'diagnosis.csv')\n",
    "test_care = pd.read_csv(TEST_DIR / 'care.csv')\n",
    "\n",
    "# Use the improved feature engineering from 03_improved_model_training.py\n",
    "# (Import or copy the engineer_focused_features function)\n",
    "# For now, let's create a simplified version here\n",
    "\n",
    "print(\"✓ Data loaded\")\n",
    "\n",
    "# Visualization: Data Overview\n",
    "print(\"\\n  Creating data overview visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 1: Data Loading - Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Data shapes\n",
    "data_shapes = {\n",
    "    'patient': len(train_patient),\n",
    "    'visit': len(train_visit),\n",
    "    'diagnosis': len(train_diagnosis),\n",
    "    'care': len(train_care),\n",
    "    'risk': len(train_risk)\n",
    "}\n",
    "axes[0, 0].bar(data_shapes.keys(), data_shapes.values(), color='steelblue')\n",
    "axes[0, 0].set_title('Training Data Record Counts', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Records')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(data_shapes.values()):\n",
    "    axes[0, 0].text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Missing values\n",
    "train_data_list = [train_patient, train_visit, train_diagnosis, train_care, train_risk]\n",
    "train_data_names = ['patient', 'visit', 'diagnosis', 'care', 'risk']\n",
    "missing_counts = [df.isnull().sum().sum() for df in train_data_list]\n",
    "axes[0, 1].bar(train_data_names, missing_counts, color='coral')\n",
    "axes[0, 1].set_title('Missing Values Count', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Missing Values')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Age distribution\n",
    "axes[1, 0].hist(train_patient['age'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
    "axes[1, 0].set_title('Age Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Risk score distribution\n",
    "axes[1, 1].hist(train_risk['risk_score'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[1, 1].set_title('Risk Score Distribution (Target)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Risk Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '01_data_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/01_data_overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cdea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Analyzing target distribution for transformation...\n",
      "  Target Skewness: 8.65\n",
      "  Target Kurtosis: 142.04\n",
      "  original: Correlation = 0.293\n",
      "  log1p: Correlation = 0.503\n",
      "  sqrt: Correlation = 0.465\n",
      "✓ Target analysis complete\n",
      "\n",
      "  Creating target transformation visualizations...\n",
      "  ✓ Saved: analysis_outputs/02_target_transformation.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Analyzing target distribution for transformation...\")\n",
    "\n",
    "# Check target skewness\n",
    "target_skew = train_risk['risk_score'].skew()\n",
    "target_kurt = train_risk['risk_score'].kurtosis()\n",
    "\n",
    "print(f\"  Target Skewness: {target_skew:.2f}\")\n",
    "print(f\"  Target Kurtosis: {target_kurt:.2f}\")\n",
    "\n",
    "# Try different transformations\n",
    "transformations = {\n",
    "    'original': lambda x: x,\n",
    "    'log1p': lambda x: np.log1p(x),\n",
    "    'sqrt': lambda x: np.sqrt(x),\n",
    "    'box_cox': None  # Will use PowerTransformer\n",
    "}\n",
    "\n",
    "best_transformation = 'original'\n",
    "best_score = float('inf')\n",
    "\n",
    "# Test transformations with simple model\n",
    "X_sample = train_patient[['age']].fillna(train_patient['age'].median())\n",
    "y_sample = train_risk['risk_score']\n",
    "\n",
    "for trans_name, trans_func in transformations.items():\n",
    "    if trans_func is None:\n",
    "        continue\n",
    "    try:\n",
    "        y_transformed = trans_func(y_sample)\n",
    "        # Simple correlation test\n",
    "        corr = abs(X_sample['age'].corr(y_transformed))\n",
    "        if corr > 0.1:  # Some correlation exists\n",
    "            print(f\"  {trans_name}: Correlation = {corr:.3f}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"✓ Target analysis complete\")\n",
    "\n",
    "# Visualization: Target Distribution Analysis\n",
    "print(\"\\n  Creating target transformation visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 2: Target Transformation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original distribution\n",
    "axes[0, 0].hist(y_sample, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(y_sample.mean(), color='red', linestyle='--', label=f'Mean: {y_sample.mean():.2f}')\n",
    "axes[0, 0].axvline(y_sample.median(), color='green', linestyle='--', label=f'Median: {y_sample.median():.2f}')\n",
    "axes[0, 0].set_title(f'Original Distribution\\n(Skewness: {target_skew:.2f}, Kurtosis: {target_kurt:.2f})', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Risk Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-transformed distribution\n",
    "y_log = np.log1p(y_sample)\n",
    "log_skew = y_log.skew()\n",
    "log_kurt = y_log.kurtosis()\n",
    "axes[0, 1].hist(y_log, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(y_log.mean(), color='red', linestyle='--', label=f'Mean: {y_log.mean():.2f}')\n",
    "axes[0, 1].axvline(y_log.median(), color='green', linestyle='--', label=f'Median: {y_log.median():.2f}')\n",
    "axes[0, 1].set_title(f'Log1p Transformed Distribution\\n(Skewness: {log_skew:.2f}, Kurtosis: {log_kurt:.2f})', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('log1p(Risk Score)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot original\n",
    "stats.probplot(y_sample, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot: Original Distribution', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot transformed\n",
    "stats.probplot(y_log, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot: Log-Transformed Distribution', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '02_target_transformation.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/02_target_transformation.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9794c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Outlier detection and handling...\n",
      "  Risk score outliers: 653 (8.16%)\n",
      "✓ Outlier analysis complete\n",
      "\n",
      "  Creating outlier analysis visualizations...\n",
      "  ✓ Saved: analysis_outputs/03_outlier_analysis.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. Outlier detection and handling...\")\n",
    "\n",
    "def detect_outliers_iqr(data, columns):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    outliers = {}\n",
    "    for col in columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outlier_count = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()\n",
    "        outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percentage': (outlier_count / len(data)) * 100,\n",
    "            'bounds': (lower_bound, upper_bound)\n",
    "        }\n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in target\n",
    "target_outliers = detect_outliers_iqr(train_risk, ['risk_score'])\n",
    "print(f\"  Risk score outliers: {target_outliers['risk_score']['count']} ({target_outliers['risk_score']['percentage']:.2f}%)\")\n",
    "\n",
    "# Strategy: Cap extreme outliers in features, but keep in target (they're real high-risk patients)\n",
    "print(\"✓ Outlier analysis complete\")\n",
    "\n",
    "# Visualization: Outlier Analysis\n",
    "print(\"\\n  Creating outlier analysis visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Step 3: Outlier Detection and Handling', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Box plot for risk score\n",
    "risk_data = train_risk['risk_score']\n",
    "Q1 = risk_data.quantile(0.25)\n",
    "Q3 = risk_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "axes[0].boxplot(risk_data, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "axes[0].axhline(lower_bound, color='red', linestyle='--', label=f'Lower bound: {lower_bound:.2f}')\n",
    "axes[0].axhline(upper_bound, color='red', linestyle='--', label=f'Upper bound: {upper_bound:.2f}')\n",
    "axes[0].set_title(f'Risk Score Outliers\\n(Outliers: {target_outliers[\"risk_score\"][\"count\"]}, {target_outliers[\"risk_score\"][\"percentage\"]:.2f}%)', \n",
    "                  fontweight='bold')\n",
    "axes[0].set_ylabel('Risk Score')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution with outlier region highlighted\n",
    "axes[1].hist(risk_data, bins=50, color='steelblue', edgecolor='black', alpha=0.7, label='All Data')\n",
    "outliers = risk_data[(risk_data < lower_bound) | (risk_data > upper_bound)]\n",
    "if len(outliers) > 0:\n",
    "    axes[1].hist(outliers, bins=20, color='red', edgecolor='black', alpha=0.7, label='Outliers')\n",
    "axes[1].axvline(lower_bound, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].axvline(upper_bound, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].fill_betweenx([0, axes[1].get_ylim()[1]], lower_bound, upper_bound, \n",
    "                       alpha=0.2, color='green', label='Normal Range')\n",
    "axes[1].set_title('Risk Score Distribution with Outlier Bounds', fontweight='bold')\n",
    "axes[1].set_xlabel('Risk Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '03_outlier_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/03_outlier_analysis.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542bbcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Advanced feature engineering...\n",
      "  Engineering features...\n",
      "✓ Features engineered: 46 features\n",
      "\n",
      "  Creating feature engineering visualizations...\n",
      "  ✓ Saved: analysis_outputs/04_feature_engineering.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Advanced feature engineering...\")\n",
    "\n",
    "def engineer_advanced_features(patient, visit, diagnosis, care):\n",
    "    \"\"\"Advanced feature engineering with domain insights\"\"\"\n",
    "    \n",
    "    features = patient[['patient_id', 'age']].copy()\n",
    "    \n",
    "    # Age bins (more granular)\n",
    "    features['age_bin_18_35'] = ((features['age'] >= 18) & (features['age'] <= 35)).astype(int)\n",
    "    features['age_bin_36_50'] = ((features['age'] >= 36) & (features['age'] <= 50)).astype(int)\n",
    "    features['age_bin_51_65'] = ((features['age'] >= 51) & (features['age'] <= 65)).astype(int)\n",
    "    features['age_bin_65_plus'] = (features['age'] > 65).astype(int)\n",
    "    \n",
    "    # Hot spotter\n",
    "    features['is_hot_spotter'] = (patient['hot_spotter_identified_at'] != '0001-01-01').astype(int)\n",
    "    features['hot_spotter_readmission_flag'] = (patient['hot_spotter_readmission_flag'] == 't').astype(int)\n",
    "    features['hot_spotter_chronic_flag'] = (patient['hot_spotter_chronic_flag'] == 't').astype(int)\n",
    "    \n",
    "    # ===== VISIT FEATURES (Enhanced) =====\n",
    "    visit_agg = visit.groupby('patient_id').agg({\n",
    "        'visit_id': 'count',\n",
    "        'readmsn_ind': lambda x: (x == 't').sum(),\n",
    "    }).reset_index()\n",
    "    visit_agg.columns = ['patient_id', 'total_visits', 'readmission_count']\n",
    "    \n",
    "    # Visit type counts\n",
    "    visit_type_counts = visit.groupby(['patient_id', 'visit_type']).size().unstack(fill_value=0)\n",
    "    visit_type_cols = []\n",
    "    for col in visit_type_counts.columns:\n",
    "        col_name = f'visits_{col.lower().replace(\" \", \"_\")}'\n",
    "        visit_type_counts[col_name] = visit_type_counts[col]\n",
    "        visit_type_cols.append(col_name)\n",
    "    visit_type_counts = visit_type_counts[visit_type_cols]\n",
    "    \n",
    "    # Visit duration features\n",
    "    visit['visit_start_dt'] = pd.to_datetime(visit['visit_start_dt'], errors='coerce')\n",
    "    visit['visit_end_dt'] = pd.to_datetime(visit['visit_end_dt'], errors='coerce')\n",
    "    visit['visit_duration'] = (visit['visit_end_dt'] - visit['visit_start_dt']).dt.days.fillna(0).clip(0, 365)\n",
    "    \n",
    "    visit_duration_agg = visit.groupby('patient_id')['visit_duration'].agg(['mean', 'max', 'min', 'std', 'sum']).reset_index()\n",
    "    visit_duration_agg.columns = ['patient_id', 'avg_visit_duration', 'max_visit_duration', \n",
    "                                  'min_visit_duration', 'std_visit_duration', 'total_visit_days']\n",
    "    \n",
    "    # Visit frequency patterns\n",
    "    visit['visit_start_dt'] = pd.to_datetime(visit['visit_start_dt'], errors='coerce')\n",
    "    REFERENCE_DATE = pd.to_datetime('2025-03-01')\n",
    "    visit['days_since_visit'] = (REFERENCE_DATE - visit['visit_start_dt']).dt.days.fillna(365).clip(0, 365*5)\n",
    "    \n",
    "    # Recent visits\n",
    "    for days in [30, 60, 90]:\n",
    "        recent = visit[visit['days_since_visit'] <= days].groupby('patient_id').size()\n",
    "        visit_agg[f'visits_last_{days}_days'] = visit_agg['patient_id'].map(recent).fillna(0)\n",
    "    \n",
    "    # Emergency visits\n",
    "    visit['is_emergency'] = ((visit['visit_type'] == 'ER') | (visit['visit_type'] == 'INPATIENT')).astype(int)\n",
    "    emergency_agg = visit.groupby('patient_id')['is_emergency'].agg(['sum', 'mean']).reset_index()\n",
    "    emergency_agg.columns = ['patient_id', 'emergency_visit_count', 'emergency_visit_rate']\n",
    "    \n",
    "    features = features.merge(visit_agg, on='patient_id', how='left')\n",
    "    features = features.merge(visit_type_counts, on='patient_id', how='left')\n",
    "    features = features.merge(visit_duration_agg, on='patient_id', how='left')\n",
    "    features = features.merge(emergency_agg, on='patient_id', how='left')\n",
    "    \n",
    "    # Visit ratios\n",
    "    if 'total_visits' in features.columns:\n",
    "        for col in visit_type_cols:\n",
    "            if col in features.columns:\n",
    "                features[f'{col}_ratio'] = features[col] / (features['total_visits'] + 1)\n",
    "    \n",
    "    # ===== DIAGNOSIS FEATURES =====\n",
    "    diagnosis['is_chronic'] = (diagnosis['is_chronic'] == 't').astype(int) if diagnosis['is_chronic'].dtype == 'object' else diagnosis['is_chronic'].astype(int)\n",
    "    chronic_conditions = diagnosis[diagnosis['is_chronic'] == 1]\n",
    "    \n",
    "    diagnosis_agg = diagnosis.groupby('patient_id').agg({\n",
    "        'diagnosis_id': 'count',\n",
    "        'is_chronic': 'sum',\n",
    "    }).reset_index()\n",
    "    diagnosis_agg.columns = ['patient_id', 'total_diagnoses', 'chronic_count']\n",
    "    \n",
    "    # Specific conditions\n",
    "    for condition in ['CANCER', 'DIABETES', 'HYPERTENSION']:\n",
    "        condition_patients = chronic_conditions[chronic_conditions['condition_name'] == condition]['patient_id'].unique()\n",
    "        diagnosis_agg[f'has_{condition.lower()}'] = diagnosis_agg['patient_id'].isin(condition_patients).astype(int)\n",
    "    \n",
    "    # Severity score\n",
    "    diagnosis_agg['chronic_severity_score'] = (\n",
    "        diagnosis_agg['has_cancer'] * 5 +\n",
    "        diagnosis_agg['has_diabetes'] * 2 +\n",
    "        diagnosis_agg['has_hypertension'] * 1\n",
    "    )\n",
    "    \n",
    "    features = features.merge(diagnosis_agg, on='patient_id', how='left')\n",
    "    \n",
    "    # ===== CARE FEATURES =====\n",
    "    care['care_gap_ind'] = (care['care_gap_ind'] == 't').astype(int) if care['care_gap_ind'].dtype == 'object' else care['care_gap_ind'].astype(int)\n",
    "    \n",
    "    care_agg = care.groupby('patient_id').agg({\n",
    "        'care_id': 'count',\n",
    "        'care_gap_ind': 'sum',\n",
    "    }).reset_index()\n",
    "    care_agg.columns = ['patient_id', 'total_care_records', 'care_gaps']\n",
    "    care_agg['care_gap_rate'] = care_agg['care_gaps'] / (care_agg['total_care_records'] + 1)\n",
    "    \n",
    "    features = features.merge(care_agg, on='patient_id', how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'patient_id':\n",
    "            features[col] = features[col].fillna(0)\n",
    "    \n",
    "    # ===== ADVANCED INTERACTIONS =====\n",
    "    \n",
    "    # High-risk combinations\n",
    "    if 'has_cancer' in features.columns and 'visits_inpatient' in features.columns:\n",
    "        features['cancer_x_inpatient'] = features['has_cancer'] * features['visits_inpatient']\n",
    "    \n",
    "    if 'has_cancer' in features.columns and 'readmission_count' in features.columns:\n",
    "        features['cancer_x_readmission'] = features['has_cancer'] * features['readmission_count']\n",
    "    \n",
    "    if 'chronic_severity_score' in features.columns and 'emergency_visit_count' in features.columns:\n",
    "        features['chronic_x_emergency'] = features['chronic_severity_score'] * features['emergency_visit_count']\n",
    "    \n",
    "    if 'care_gap_rate' in features.columns and 'has_cancer' in features.columns:\n",
    "        features['gaps_x_cancer'] = features['care_gap_rate'] * features['has_cancer'] * 10\n",
    "    \n",
    "    # Age risk interactions\n",
    "    if 'age' in features.columns:\n",
    "        if 'has_cancer' in features.columns:\n",
    "            features['elderly_cancer'] = ((features['age'] > 65) & (features['has_cancer'] == 1)).astype(int)\n",
    "        if 'emergency_visit_count' in features.columns:\n",
    "            features['elderly_emergency'] = ((features['age'] > 65) & (features['emergency_visit_count'] > 0)).astype(int)\n",
    "    \n",
    "    # Composite high-risk flags\n",
    "    features['high_risk_flag'] = (\n",
    "        (features.get('has_cancer', 0) == 1) |\n",
    "        (features.get('visits_inpatient', 0) > 2) |\n",
    "        (features.get('readmission_count', 0) > 1) |\n",
    "        (features.get('is_hot_spotter', 0) == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Risk score components\n",
    "    features['visit_risk_component'] = (\n",
    "        features.get('emergency_visit_count', 0) * 2 +\n",
    "        features.get('readmission_count', 0) * 3\n",
    "    )\n",
    "    \n",
    "    features['chronic_risk_component'] = features.get('chronic_severity_score', 0) * 2\n",
    "    \n",
    "    features['care_risk_component'] = features.get('care_gap_rate', 0) * 10\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Engineer features\n",
    "print(\"  Engineering features...\")\n",
    "train_features = engineer_advanced_features(train_patient, train_visit, train_diagnosis, train_care)\n",
    "test_features = engineer_advanced_features(test_patient, test_visit, test_diagnosis, test_care)\n",
    "\n",
    "print(f\"✓ Features engineered: {train_features.shape[1]} features\")\n",
    "\n",
    "# Visualization: Feature Engineering Summary\n",
    "print(\"\\n  Creating feature engineering visualizations...\")\n",
    "feature_categories = {\n",
    "    'Demographics': ['age', 'age_bin_18_35', 'age_bin_36_50', 'age_bin_51_65', 'age_bin_65_plus'],\n",
    "    'Hot Spotter': ['is_hot_spotter', 'hot_spotter_readmission_flag', 'hot_spotter_chronic_flag'],\n",
    "    'Visits': [col for col in train_features.columns if 'visit' in col.lower() and col not in ['patient_id']],\n",
    "    'Diagnosis': [col for col in train_features.columns if 'diagnosis' in col.lower() or 'chronic' in col.lower() or 'cancer' in col.lower() or 'diabetes' in col.lower() or 'hypertension' in col.lower()],\n",
    "    'Care': [col for col in train_features.columns if 'care' in col.lower() or 'gap' in col.lower()],\n",
    "    'Interactions': [col for col in train_features.columns if 'x_' in col or 'elderly' in col.lower() or 'risk_component' in col.lower() or 'high_risk' in col.lower()]\n",
    "}\n",
    "category_counts = {cat: len([f for f in feats if f in train_features.columns]) for cat, feats in feature_categories.items()}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Step 4: Feature Engineering Summary', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Feature counts by category\n",
    "axes[0].bar(category_counts.keys(), category_counts.values(), color='steelblue')\n",
    "axes[0].set_title('Features by Category', fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Features')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, (cat, count) in enumerate(category_counts.items()):\n",
    "    axes[0].text(i, count, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Top 15 feature correlation with target (if available)\n",
    "train_data_viz = train_features.merge(train_risk[['patient_id', 'risk_score']], on='patient_id', how='inner')\n",
    "if 'risk_score' in train_data_viz.columns:\n",
    "    numeric_features = train_data_viz.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'patient_id' in numeric_features:\n",
    "        numeric_features.remove('patient_id')\n",
    "    if 'risk_score' in numeric_features:\n",
    "        numeric_features.remove('risk_score')\n",
    "    \n",
    "    if len(numeric_features) > 0:\n",
    "        correlations = train_data_viz[numeric_features + ['risk_score']].corr()['risk_score'].abs().sort_values(ascending=False)\n",
    "        top_features = correlations.head(15).index.tolist()\n",
    "        if 'risk_score' in top_features:\n",
    "            top_features.remove('risk_score')\n",
    "        top_corrs = correlations[top_features].sort_values(ascending=True)\n",
    "        \n",
    "        axes[1].barh(range(len(top_corrs)), top_corrs.values, color='coral')\n",
    "        axes[1].set_yticks(range(len(top_corrs)))\n",
    "        axes[1].set_yticklabels([f[:30] for f in top_corrs.index])  # Truncate long names\n",
    "        axes[1].set_title('Top 15 Feature Correlations with Risk Score', fontweight='bold')\n",
    "        axes[1].set_xlabel('Absolute Correlation')\n",
    "        axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '04_feature_engineering.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/04_feature_engineering.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4545937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Preparing data with transformations...\n",
      "✓ Training set: (6400, 45)\n",
      "✓ Validation set: (1600, 45)\n",
      "\n",
      "  Testing target transformations...\n",
      "  Original skewness: 9.16\n",
      "  Log-transformed skewness: 1.35\n",
      "  Will use log transformation: True\n",
      "\n",
      "  Creating data preparation visualizations...\n",
      "  ✓ Saved: analysis_outputs/05_data_preparation.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Preparing data with transformations...\")\n",
    "\n",
    "train_data = train_features.merge(train_risk[['patient_id', 'risk_score']], on='patient_id', how='inner')\n",
    "\n",
    "feature_cols = [col for col in train_data.columns if col not in ['patient_id', 'risk_score']]\n",
    "\n",
    "X = train_data[feature_cols].fillna(0)\n",
    "y = train_data['risk_score']\n",
    "\n",
    "# Handle outliers in features (cap extreme values)\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in [np.int64, np.float64]:\n",
    "        Q1, Q3 = X[col].quantile([0.01, 0.99])\n",
    "        X[col] = X[col].clip(lower=Q1, upper=Q3)\n",
    "\n",
    "# Replace inf and large values\n",
    "X = X.replace([np.inf, -np.inf], 0)\n",
    "X = X.clip(lower=-1000, upper=1000)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"✓ Training set: {X_train.shape}\")\n",
    "print(f\"✓ Validation set: {X_val.shape}\")\n",
    "\n",
    "# Try log transformation for target\n",
    "print(\"\\n  Testing target transformations...\")\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "# Compare original vs log-transformed\n",
    "orig_skew = y_train.skew()\n",
    "log_skew = y_train_log.skew()\n",
    "\n",
    "print(f\"  Original skewness: {orig_skew:.2f}\")\n",
    "print(f\"  Log-transformed skewness: {log_skew:.2f}\")\n",
    "\n",
    "use_log_transform = abs(log_skew) < abs(orig_skew)\n",
    "print(f\"  Will use log transformation: {use_log_transform}\")\n",
    "\n",
    "if use_log_transform:\n",
    "    y_train_model = y_train_log\n",
    "    y_val_model = y_val_log\n",
    "else:\n",
    "    y_train_model = y_train\n",
    "    y_val_model = y_val\n",
    "\n",
    "# Visualization: Data Preparation\n",
    "print(\"\\n  Creating data preparation visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 5: Data Preparation and Transformation', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Train/Val split visualization\n",
    "split_data = {'Train': len(y_train), 'Validation': len(y_val)}\n",
    "axes[0, 0].pie(split_data.values(), labels=split_data.keys(), autopct='%1.1f%%', \n",
    "               colors=['steelblue', 'coral'], startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[0, 0].set_title(f'Train/Validation Split\\n(Total: {len(y_train) + len(y_val)} samples)', fontweight='bold')\n",
    "\n",
    "# Target distribution comparison\n",
    "axes[0, 1].hist(y_train, bins=30, alpha=0.7, label='Train', color='steelblue', edgecolor='black')\n",
    "axes[0, 1].hist(y_val, bins=30, alpha=0.7, label='Validation', color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Target Distribution: Train vs Validation', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Risk Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Transformation comparison\n",
    "axes[1, 0].hist(y_train_model if use_log_transform else y_train, bins=30, \n",
    "                color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "transform_type = 'log1p' if use_log_transform else 'original'\n",
    "axes[1, 0].set_title(f'Transformed Target Distribution\\n(Using: {transform_type})', fontweight='bold')\n",
    "axes[1, 0].set_xlabel(f'Risk Score ({transform_type})')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature count by type\n",
    "numeric_features_count = X.select_dtypes(include=[np.number]).shape[1]\n",
    "axes[1, 1].bar(['Numeric Features'], [numeric_features_count], color='skyblue', width=0.5)\n",
    "axes[1, 1].text(0, numeric_features_count, str(numeric_features_count), \n",
    "               ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_title(f'Final Feature Set\\n({numeric_features_count} features)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '05_data_preparation.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/05_data_preparation.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d24378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 19:18:05,047] A new study created in memory with name: xgb_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Hyperparameter optimization...\n",
      "  Using Optuna for hyperparameter tuning...\n",
      "  Optimizing XGBoost...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e542bbf928484551bbbfebdb19d01e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 19:18:08,865] Trial 0 finished with value: 0.3490297466062015 and parameters: {'max_depth': 7, 'learning_rate': 0.014265211206474893, 'n_estimators': 1449, 'subsample': 0.9784851353076265, 'colsample_bytree': 0.7433723152888884, 'min_child_weight': 5, 'reg_alpha': 0.028972394179410017, 'reg_lambda': 0.05131065749785435}. Best is trial 0 with value: 0.3490297466062015.\n",
      "[I 2025-11-02 19:18:11,693] Trial 1 finished with value: 0.3523530643984378 and parameters: {'max_depth': 4, 'learning_rate': 0.07738545976122886, 'n_estimators': 1112, 'subsample': 0.6791051480358563, 'colsample_bytree': 0.706027891517449, 'min_child_weight': 6, 'reg_alpha': 0.011510559099347964, 'reg_lambda': 0.017360301627220458}. Best is trial 0 with value: 0.3490297466062015.\n",
      "[I 2025-11-02 19:18:14,166] Trial 2 finished with value: 0.33316723083315836 and parameters: {'max_depth': 4, 'learning_rate': 0.03847854609472188, 'n_estimators': 608, 'subsample': 0.7498529843841016, 'colsample_bytree': 0.6065949058712927, 'min_child_weight': 10, 'reg_alpha': 1.9146594556129948, 'reg_lambda': 0.04654651087978576}. Best is trial 2 with value: 0.33316723083315836.\n",
      "[I 2025-11-02 19:18:14,820] Trial 3 finished with value: 0.3331663531885891 and parameters: {'max_depth': 7, 'learning_rate': 0.02110134618181159, 'n_estimators': 218, 'subsample': 0.885299320173661, 'colsample_bytree': 0.8900082888039482, 'min_child_weight': 10, 'reg_alpha': 0.016382663206270052, 'reg_lambda': 0.1427608185218629}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:15,537] Trial 4 finished with value: 0.333582414730119 and parameters: {'max_depth': 7, 'learning_rate': 0.017060168033947207, 'n_estimators': 253, 'subsample': 0.6352006172556145, 'colsample_bytree': 0.8185432338674761, 'min_child_weight': 4, 'reg_alpha': 0.07576707761954772, 'reg_lambda': 0.10720779256492302}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:17,492] Trial 5 finished with value: 0.3355434683571239 and parameters: {'max_depth': 6, 'learning_rate': 0.011780677415507354, 'n_estimators': 1007, 'subsample': 0.8490500322141853, 'colsample_bytree': 0.7929094319790158, 'min_child_weight': 7, 'reg_alpha': 1.1972508377833067, 'reg_lambda': 0.10902502953831392}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:19,818] Trial 6 finished with value: 0.35228470947207174 and parameters: {'max_depth': 9, 'learning_rate': 0.047353129678076276, 'n_estimators': 803, 'subsample': 0.6018601122791883, 'colsample_bytree': 0.6885099742984582, 'min_child_weight': 1, 'reg_alpha': 0.8832213701498108, 'reg_lambda': 0.03894119748537371}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:22,101] Trial 7 finished with value: 0.3510917070261305 and parameters: {'max_depth': 10, 'learning_rate': 0.015281591014313338, 'n_estimators': 718, 'subsample': 0.7880879903695583, 'colsample_bytree': 0.69049023124601, 'min_child_weight': 4, 'reg_alpha': 0.02033575402010807, 'reg_lambda': 0.25781787597754285}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:23,600] Trial 8 finished with value: 0.3442689820553044 and parameters: {'max_depth': 5, 'learning_rate': 0.08616826077030568, 'n_estimators': 958, 'subsample': 0.9012508350977007, 'colsample_bytree': 0.7343813279183321, 'min_child_weight': 2, 'reg_alpha': 1.5817099245424264, 'reg_lambda': 0.1806917164471229}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:27,083] Trial 9 finished with value: 0.3586946709698288 and parameters: {'max_depth': 8, 'learning_rate': 0.02307496234128575, 'n_estimators': 1334, 'subsample': 0.6067133828711613, 'colsample_bytree': 0.6838186479014735, 'min_child_weight': 4, 'reg_alpha': 0.042612223574904504, 'reg_lambda': 0.5022827419800738}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:27,783] Trial 10 finished with value: 0.33661970613953573 and parameters: {'max_depth': 8, 'learning_rate': 0.031155271265798503, 'n_estimators': 249, 'subsample': 0.9889734374017981, 'colsample_bytree': 0.9528729007558863, 'min_child_weight': 10, 'reg_alpha': 0.22961938617415184, 'reg_lambda': 1.4994563378265426}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:28,679] Trial 11 finished with value: 0.3491537053529856 and parameters: {'max_depth': 4, 'learning_rate': 0.154096143095924, 'n_estimators': 543, 'subsample': 0.7532093638851411, 'colsample_bytree': 0.607293347344086, 'min_child_weight': 10, 'reg_alpha': 0.2734393006970953, 'reg_lambda': 0.011730829754988799}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:30,049] Trial 12 finished with value: 0.34146791252932446 and parameters: {'max_depth': 6, 'learning_rate': 0.038749294214978684, 'n_estimators': 502, 'subsample': 0.7271402418428605, 'colsample_bytree': 0.9241928881769089, 'min_child_weight': 8, 'reg_alpha': 0.44622458590884495, 'reg_lambda': 0.0352951784660647}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:30,883] Trial 13 finished with value: 0.3417281000763705 and parameters: {'max_depth': 5, 'learning_rate': 0.057608086177430046, 'n_estimators': 459, 'subsample': 0.8855433685772707, 'colsample_bytree': 0.8987315635130928, 'min_child_weight': 9, 'reg_alpha': 0.09688816049151322, 'reg_lambda': 0.4989888536478634}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:33,024] Trial 14 finished with value: 0.34740664553093287 and parameters: {'max_depth': 10, 'learning_rate': 0.025626707955480575, 'n_estimators': 672, 'subsample': 0.8214977013979657, 'colsample_bytree': 0.8513848406224265, 'min_child_weight': 8, 'reg_alpha': 0.6361125049673795, 'reg_lambda': 0.05987266481270906}. Best is trial 3 with value: 0.3331663531885891.\n",
      "[I 2025-11-02 19:18:33,867] Trial 15 finished with value: 0.3327045278489987 and parameters: {'max_depth': 5, 'learning_rate': 0.0199509861808729, 'n_estimators': 354, 'subsample': 0.9236642202933703, 'colsample_bytree': 0.9839697376493699, 'min_child_weight': 10, 'reg_alpha': 0.05959246726895416, 'reg_lambda': 0.02593353429449039}. Best is trial 15 with value: 0.3327045278489987.\n",
      "[I 2025-11-02 19:18:34,784] Trial 16 finished with value: 0.3347678829287893 and parameters: {'max_depth': 6, 'learning_rate': 0.020597378044315794, 'n_estimators': 373, 'subsample': 0.9273450939748352, 'colsample_bytree': 0.9872700879273091, 'min_child_weight': 8, 'reg_alpha': 0.010897537515466186, 'reg_lambda': 0.020925869720140815}. Best is trial 15 with value: 0.3327045278489987.\n",
      "[I 2025-11-02 19:18:35,830] Trial 17 finished with value: 0.334220527800736 and parameters: {'max_depth': 8, 'learning_rate': 0.012055795026382411, 'n_estimators': 330, 'subsample': 0.938756492143206, 'colsample_bytree': 0.8773419653535026, 'min_child_weight': 9, 'reg_alpha': 0.04870766738298904, 'reg_lambda': 0.3489149730573991}. Best is trial 15 with value: 0.3327045278489987.\n",
      "[I 2025-11-02 19:18:36,433] Trial 18 finished with value: 0.33213265988633534 and parameters: {'max_depth': 5, 'learning_rate': 0.0289730820951581, 'n_estimators': 215, 'subsample': 0.8588459976030566, 'colsample_bytree': 0.9856249944384196, 'min_child_weight': 7, 'reg_alpha': 0.024361757902916784, 'reg_lambda': 1.1297080053013513}. Best is trial 18 with value: 0.33213265988633534.\n",
      "[I 2025-11-02 19:18:37,260] Trial 19 finished with value: 0.3592767175402402 and parameters: {'max_depth': 5, 'learning_rate': 0.18671934261067144, 'n_estimators': 403, 'subsample': 0.8413810975057054, 'colsample_bytree': 0.9968240045920322, 'min_child_weight': 6, 'reg_alpha': 0.1274706924602019, 'reg_lambda': 1.6643407809809965}. Best is trial 18 with value: 0.33213265988633534.\n",
      "  ✓ Best XGBoost RMSE: 0.3321\n",
      "  ✓ Best params: {'max_depth': 5, 'learning_rate': 0.0289730820951581, 'n_estimators': 215, 'subsample': 0.8588459976030566, 'colsample_bytree': 0.9856249944384196, 'min_child_weight': 7, 'reg_alpha': 0.024361757902916784, 'reg_lambda': 1.1297080053013513, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42, 'verbosity': 0}\n",
      "\n",
      "  Creating hyperparameter optimization visualizations...\n",
      "  ✓ Saved: analysis_outputs/06_hyperparameter_optimization.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. Hyperparameter optimization...\")\n",
    "\n",
    "if OPTUNA_AVAILABLE:\n",
    "    print(\"  Using Optuna for hyperparameter tuning...\")\n",
    "    \n",
    "    def objective_xgb(trial):\n",
    "        \"\"\"XGBoost objective function\"\"\"\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 1500),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 2.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 2.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        scores = cross_val_score(model, X_train, y_train_model, \n",
    "                                cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                n_jobs=-1)\n",
    "        return -scores.mean()\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"  Optimizing XGBoost...\")\n",
    "    study_xgb = optuna.create_study(direction='minimize', study_name='xgb_optimization')\n",
    "    study_xgb.optimize(objective_xgb, n_trials=20, show_progress_bar=True)\n",
    "    \n",
    "    best_xgb_params = study_xgb.best_params\n",
    "    best_xgb_params['objective'] = 'reg:squarederror'\n",
    "    best_xgb_params['eval_metric'] = 'rmse'\n",
    "    best_xgb_params['random_state'] = 42\n",
    "    best_xgb_params['verbosity'] = 0\n",
    "    \n",
    "    print(f\"  ✓ Best XGBoost RMSE: {study_xgb.best_value:.4f}\")\n",
    "    print(f\"  ✓ Best params: {best_xgb_params}\")\n",
    "    \n",
    "    # Visualization: Optuna Optimization History\n",
    "    print(\"\\n  Creating hyperparameter optimization visualizations...\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Step 6: Hyperparameter Optimization (Optuna)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Optimization history\n",
    "    opt_history = study_xgb.trials_dataframe()\n",
    "    axes[0].plot(range(len(opt_history)), opt_history['value'], marker='o', \n",
    "                 color='steelblue', linewidth=2, markersize=4)\n",
    "    axes[0].axhline(study_xgb.best_value, color='red', linestyle='--', \n",
    "                   label=f'Best RMSE: {study_xgb.best_value:.4f}', linewidth=2)\n",
    "    axes[0].set_title('Optimization History', fontweight='bold')\n",
    "    axes[0].set_xlabel('Trial Number')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter importance (if available)\n",
    "    try:\n",
    "        param_importance = optuna.importance.get_param_importances(study_xgb)\n",
    "        if param_importance:\n",
    "            params = list(param_importance.keys())\n",
    "            importances = list(param_importance.values())\n",
    "            axes[1].barh(range(len(params)), importances, color='coral')\n",
    "            axes[1].set_yticks(range(len(params)))\n",
    "            axes[1].set_yticklabels(params)\n",
    "            axes[1].set_title('Parameter Importance', fontweight='bold')\n",
    "            axes[1].set_xlabel('Importance')\n",
    "            axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    except:\n",
    "        axes[1].text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes,\n",
    "                    fontsize=12, style='italic')\n",
    "        axes[1].set_title('Parameter Importance', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / '06_hyperparameter_optimization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  ✓ Saved: analysis_outputs/06_hyperparameter_optimization.png\")\n",
    "else:\n",
    "    print(\"  Optuna not available, using optimized defaults...\")\n",
    "    best_xgb_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 7,\n",
    "        'learning_rate': 0.03,\n",
    "        'n_estimators': 1000,\n",
    "        'subsample': 0.85,\n",
    "        'colsample_bytree': 0.85,\n",
    "        'min_child_weight': 2,\n",
    "        'reg_alpha': 0.2,\n",
    "        'reg_lambda': 2,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c2e1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Training optimized models...\n",
      "  Training XGBoost...\n",
      "    ✓ XGBoost RMSE: 1.8050 (trained in 0.35s)\n",
      "  Training LightGBM...\n",
      "    ✓ LightGBM RMSE: 1.8492 (trained in 3.00s)\n",
      "\n",
      "  Creating model training visualizations...\n",
      "  ✓ Saved: analysis_outputs/07_model_training.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n7. Training optimized models...\")\n",
    "\n",
    "models_optimized = {}\n",
    "predictions_val_opt = {}\n",
    "\n",
    "# XGBoost (optimized)\n",
    "print(\"  Training XGBoost...\")\n",
    "start = time.time()\n",
    "xgb_opt = xgb.XGBRegressor(**best_xgb_params)\n",
    "xgb_opt.fit(X_train, y_train_model,\n",
    "            eval_set=[(X_train, y_train_model), (X_val, y_val_model)],\n",
    "            verbose=False)\n",
    "models_optimized['xgb'] = xgb_opt\n",
    "\n",
    "pred_val_xgb = xgb_opt.predict(X_val)\n",
    "if use_log_transform:\n",
    "    pred_val_xgb = np.expm1(pred_val_xgb)\n",
    "predictions_val_opt['xgb'] = pred_val_xgb\n",
    "\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, pred_val_xgb))\n",
    "print(f\"    ✓ XGBoost RMSE: {xgb_rmse:.4f} (trained in {time.time()-start:.2f}s)\")\n",
    "\n",
    "# LightGBM (optimized)\n",
    "print(\"  Training LightGBM...\")\n",
    "start = time.time()\n",
    "lgb_opt = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    max_depth=7,\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=1000,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    min_child_samples=2,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=2,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "lgb_opt.fit(X_train, y_train_model,\n",
    "           eval_set=[(X_train, y_train_model), (X_val, y_val_model)],\n",
    "           callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                     lgb.log_evaluation(period=0)])\n",
    "models_optimized['lgb'] = lgb_opt\n",
    "\n",
    "pred_val_lgb = lgb_opt.predict(X_val)\n",
    "if use_log_transform:\n",
    "    pred_val_lgb = np.expm1(pred_val_lgb)\n",
    "predictions_val_opt['lgb'] = pred_val_lgb\n",
    "\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_val, pred_val_lgb))\n",
    "print(f\"    ✓ LightGBM RMSE: {lgb_rmse:.4f} (trained in {time.time()-start:.2f}s)\")\n",
    "\n",
    "# Visualization: Model Training Results\n",
    "print(\"\\n  Creating model training visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 7: Model Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Predictions vs Actual - XGBoost\n",
    "axes[0, 0].scatter(y_val, pred_val_xgb, alpha=0.5, color='steelblue', s=20)\n",
    "axes[0, 0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_title(f'XGBoost: Predictions vs Actual\\n(RMSE: {xgb_rmse:.4f})', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Actual Risk Score')\n",
    "axes[0, 0].set_ylabel('Predicted Risk Score')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual - LightGBM\n",
    "axes[0, 1].scatter(y_val, pred_val_lgb, alpha=0.5, color='coral', s=20)\n",
    "axes[0, 1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_title(f'LightGBM: Predictions vs Actual\\n(RMSE: {lgb_rmse:.4f})', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Actual Risk Score')\n",
    "axes[0, 1].set_ylabel('Predicted Risk Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals - XGBoost\n",
    "residuals_xgb = y_val - pred_val_xgb\n",
    "axes[1, 0].scatter(pred_val_xgb, residuals_xgb, alpha=0.5, color='steelblue', s=20)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_title('XGBoost: Residuals Plot', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Predicted Risk Score')\n",
    "axes[1, 0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals - LightGBM\n",
    "residuals_lgb = y_val - pred_val_lgb\n",
    "axes[1, 1].scatter(pred_val_lgb, residuals_lgb, alpha=0.5, color='coral', s=20)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_title('LightGBM: Residuals Plot', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Predicted Risk Score')\n",
    "axes[1, 1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '07_model_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/07_model_training.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c37dfe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Creating stacking model...\n",
      "  ✓ Stacking RMSE: 1.7698\n",
      "\n",
      "  Creating stacking model visualizations...\n",
      "  ✓ Saved: analysis_outputs/08_stacking_model.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n8. Creating stacking model...\")\n",
    "\n",
    "# Use predictions from base models as features for meta-model\n",
    "X_val_stacked = pd.DataFrame({\n",
    "    'xgb_pred': predictions_val_opt['xgb'],\n",
    "    'lgb_pred': predictions_val_opt['lgb'],\n",
    "})\n",
    "\n",
    "# Train meta-model (XGBoost)\n",
    "meta_model = xgb.XGBRegressor(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Get train predictions for stacking\n",
    "X_train_stacked = pd.DataFrame({\n",
    "    'xgb_pred': xgb_opt.predict(X_train),\n",
    "    'lgb_pred': lgb_opt.predict(X_train),\n",
    "})\n",
    "\n",
    "if use_log_transform:\n",
    "    X_train_stacked['xgb_pred'] = np.expm1(X_train_stacked['xgb_pred'])\n",
    "    X_train_stacked['lgb_pred'] = np.expm1(X_train_stacked['lgb_pred'])\n",
    "\n",
    "meta_model.fit(X_train_stacked, y_train)\n",
    "\n",
    "# Stacking predictions\n",
    "y_val_stacked = meta_model.predict(X_val_stacked)\n",
    "stacked_rmse = np.sqrt(mean_squared_error(y_val, y_val_stacked))\n",
    "\n",
    "print(f\"  ✓ Stacking RMSE: {stacked_rmse:.4f}\")\n",
    "\n",
    "models_optimized['stacked'] = {\n",
    "    'base_models': models_optimized,\n",
    "    'meta_model': meta_model,\n",
    "    'use_log_transform': use_log_transform\n",
    "}\n",
    "\n",
    "# Visualization: Stacking Model\n",
    "print(\"\\n  Creating stacking model visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Step 8: Stacking Model', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Base model predictions correlation\n",
    "axes[0].scatter(predictions_val_opt['xgb'], predictions_val_opt['lgb'], \n",
    "                alpha=0.5, color='steelblue', s=20)\n",
    "corr_coef = np.corrcoef(predictions_val_opt['xgb'], predictions_val_opt['lgb'])[0, 1]\n",
    "axes[0].set_title(f'Base Model Predictions Correlation\\n(Correlation: {corr_coef:.3f})', fontweight='bold')\n",
    "axes[0].set_xlabel('XGBoost Predictions')\n",
    "axes[0].set_ylabel('LightGBM Predictions')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stacking predictions vs Actual\n",
    "axes[1].scatter(y_val, y_val_stacked, alpha=0.5, color='coral', s=20)\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_title(f'Stacked Model: Predictions vs Actual\\n(RMSE: {stacked_rmse:.4f})', fontweight='bold')\n",
    "axes[1].set_xlabel('Actual Risk Score')\n",
    "axes[1].set_ylabel('Predicted Risk Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '08_stacking_model.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/08_stacking_model.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afd25e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Model Comparison:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                RMSE            MAE             R²             \n",
      "--------------------------------------------------------------------------------\n",
      "XGB                  1.8050          0.7838          0.4649         \n",
      "LGB                  1.8492          0.7928          0.4384         \n",
      "STACKED              1.7698          0.8214          0.4856         \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ Best model: STACKED with RMSE: 1.7698\n",
      "\n",
      "  Creating model comparison visualizations...\n",
      "  ✓ Saved: analysis_outputs/09_model_comparison.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n9. Model Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<20} {'RMSE':<15} {'MAE':<15} {'R²':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, pred in predictions_val_opt.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    print(f\"{model_name.upper():<20} {rmse:<15.4f} {mae:<15.4f} {r2:<15.4f}\")\n",
    "\n",
    "print(f\"{'STACKED':<20} {stacked_rmse:<15.4f} {mean_absolute_error(y_val, y_val_stacked):<15.4f} {r2_score(y_val, y_val_stacked):<15.4f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Select best model\n",
    "all_results = {name: np.sqrt(mean_squared_error(y_val, pred)) for name, pred in predictions_val_opt.items()}\n",
    "all_results['stacked'] = stacked_rmse\n",
    "best_model_name = min(all_results, key=all_results.get)\n",
    "best_rmse = all_results[best_model_name]\n",
    "\n",
    "print(f\"\\n✓ Best model: {best_model_name.upper()} with RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# Visualization: Model Comparison\n",
    "print(\"\\n  Creating model comparison visualizations...\")\n",
    "model_results = {}\n",
    "for model_name, pred in predictions_val_opt.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    model_results[model_name.upper()] = {'RMSE': rmse, 'MAE': mae, 'R²': r2}\n",
    "\n",
    "model_results['STACKED'] = {\n",
    "    'RMSE': stacked_rmse,\n",
    "    'MAE': mean_absolute_error(y_val, y_val_stacked),\n",
    "    'R²': r2_score(y_val, y_val_stacked)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Step 9: Model Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "models = list(model_results.keys())\n",
    "rmse_values = [model_results[m]['RMSE'] for m in models]\n",
    "mae_values = [model_results[m]['MAE'] for m in models]\n",
    "r2_values = [model_results[m]['R²'] for m in models]\n",
    "\n",
    "# RMSE comparison\n",
    "colors = ['red' if m == best_model_name.upper() else 'steelblue' for m in models]\n",
    "axes[0].bar(models, rmse_values, color=colors, alpha=0.7)\n",
    "axes[0].set_title('RMSE Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, rmse) in enumerate(zip(models, rmse_values)):\n",
    "    axes[0].text(i, rmse, f'{rmse:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(models, mae_values, color=colors, alpha=0.7)\n",
    "axes[1].set_title('MAE Comparison', fontweight='bold')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, mae) in enumerate(zip(models, mae_values)):\n",
    "    axes[1].text(i, mae, f'{mae:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# R² comparison\n",
    "axes[2].bar(models, r2_values, color=colors, alpha=0.7)\n",
    "axes[2].set_title('R² Score Comparison', fontweight='bold')\n",
    "axes[2].set_ylabel('R² Score')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, r2) in enumerate(zip(models, r2_values)):\n",
    "    axes[2].text(i, r2, f'{r2:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '09_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/09_model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217aca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Analyzing feature importance...\n",
      "\n",
      "  Top 30 Features:\n",
      "  ------------------------------------------------------------\n",
      "  42. high_risk_flag                               0.3622\n",
      "  33. total_care_records                           0.1077\n",
      "  27. total_diagnoses                              0.0994\n",
      "  34. care_gaps                                    0.0873\n",
      "  28. chronic_count                                0.0632\n",
      "   1. age                                          0.0255\n",
      "  22. emergency_visit_count                        0.0254\n",
      "  38. chronic_x_emergency                          0.0165\n",
      "  18. max_visit_duration                           0.0163\n",
      "   6. is_hot_spotter                               0.0161\n",
      "  44. chronic_risk_component                       0.0146\n",
      "  43. visit_risk_component                         0.0143\n",
      "  30. has_diabetes                                 0.0125\n",
      "   9. total_visits                                 0.0121\n",
      "   5. age_bin_65_plus                              0.0118\n",
      "  32. chronic_severity_score                       0.0116\n",
      "  15. visits_inpatient                             0.0091\n",
      "  41. elderly_emergency                            0.0078\n",
      "  25. visits_inpatient_ratio                       0.0077\n",
      "  17. avg_visit_duration                           0.0071\n",
      "  20. std_visit_duration                           0.0071\n",
      "  14. visits_er                                    0.0068\n",
      "  23. emergency_visit_rate                         0.0067\n",
      "   4. age_bin_51_65                                0.0066\n",
      "  21. total_visit_days                             0.0061\n",
      "  24. visits_er_ratio                              0.0052\n",
      "  29. has_cancer                                   0.0043\n",
      "   3. age_bin_36_50                                0.0042\n",
      "   8. hot_spotter_chronic_flag                     0.0039\n",
      "  12. visits_last_60_days                          0.0034\n",
      "\n",
      "  Creating feature importance visualizations...\n",
      "  ✓ Saved: analysis_outputs/10_feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n10. Analyzing feature importance...\")\n",
    "\n",
    "# Get importance from XGBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_opt.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n  Top 30 Features:\")\n",
    "print(\"  \" + \"-\" * 60)\n",
    "for i, row in feature_importance.head(30).iterrows():\n",
    "    print(f\"  {i+1:2d}. {row['feature']:40s} {row['importance']:>10.4f}\")\n",
    "\n",
    "# Save importance\n",
    "feature_importance.to_csv(OUTPUT_DIR / 'feature_importance_advanced.csv', index=False)\n",
    "\n",
    "# Visualization: Feature Importance\n",
    "print(\"\\n  Creating feature importance visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle('Step 10: Feature Importance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Top 20 features\n",
    "top_features = feature_importance.head(20)\n",
    "axes[0].barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\n",
    "axes[0].set_yticks(range(len(top_features)))\n",
    "axes[0].set_yticklabels([f[:40] for f in top_features['feature'].values], fontsize=9)\n",
    "axes[0].set_title('Top 20 Most Important Features', fontweight='bold')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Importance distribution\n",
    "axes[1].hist(feature_importance['importance'], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(feature_importance['importance'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {feature_importance[\"importance\"].mean():.4f}', linewidth=2)\n",
    "axes[1].axvline(feature_importance['importance'].median(), color='green', linestyle='--', \n",
    "                label=f'Median: {feature_importance[\"importance\"].median():.4f}', linewidth=2)\n",
    "axes[1].set_title('Feature Importance Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '10_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/10_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b2604aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. Generating final predictions...\n",
      "\n",
      "✓ Predictions saved to C:\\Users\\himan\\Downloads\\Hilabs_Hackathon-main\\Hilabs_Hackathon-main\\predictions.csv\n",
      "\n",
      "  Prediction statistics:\n",
      "    Mean: 1.67\n",
      "    Median: 1.14\n",
      "    Std: 2.04\n",
      "    Min: 0.38\n",
      "    Max: 21.95\n",
      "\n",
      "  Creating final predictions visualizations...\n",
      "  ✓ Saved: analysis_outputs/11_final_predictions.png\n",
      "\n",
      "✓ Models saved!\n",
      "\n",
      "================================================================================\n",
      "MODEL TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Best Model: STACKED\n",
      "Best RMSE: 1.7698\n",
      "Predictions: C:\\Users\\himan\\Downloads\\Hilabs_Hackathon-main\\Hilabs_Hackathon-main\\predictions.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n11. Generating final predictions...\")\n",
    "\n",
    "# Prepare test features\n",
    "X_test = test_features[feature_cols].fillna(0)\n",
    "\n",
    "# Handle outliers and inf\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].dtype in [np.int64, np.float64]:\n",
    "        Q1, Q3 = X_test[col].quantile([0.01, 0.99])\n",
    "        X_test[col] = X_test[col].clip(lower=Q1, upper=Q3)\n",
    "X_test = X_test.replace([np.inf, -np.inf], 0).clip(-1000, 1000)\n",
    "\n",
    "# Generate predictions with best model\n",
    "if best_model_name == 'stacked':\n",
    "    # Stacking predictions\n",
    "    pred_xgb_test = xgb_opt.predict(X_test)\n",
    "    pred_lgb_test = lgb_opt.predict(X_test)\n",
    "    \n",
    "    if use_log_transform:\n",
    "        pred_xgb_test = np.expm1(pred_xgb_test)\n",
    "        pred_lgb_test = np.expm1(pred_lgb_test)\n",
    "    \n",
    "    X_test_stacked = pd.DataFrame({\n",
    "        'xgb_pred': pred_xgb_test,\n",
    "        'lgb_pred': pred_lgb_test,\n",
    "    })\n",
    "    test_predictions = meta_model.predict(X_test_stacked)\n",
    "else:\n",
    "    pred = models_optimized[best_model_name].predict(X_test)\n",
    "    if use_log_transform:\n",
    "        test_predictions = np.expm1(pred)\n",
    "    else:\n",
    "        test_predictions = pred\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'patient_id': test_features['patient_id'],\n",
    "    'predicted_risk_score': test_predictions\n",
    "})\n",
    "\n",
    "# Post-processing: Ensure reasonable range\n",
    "submission['predicted_risk_score'] = submission['predicted_risk_score'].clip(lower=0.1, upper=100)\n",
    "\n",
    "# Save\n",
    "submission_file = BASE_DIR / 'predictions.csv'\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Predictions saved to {submission_file}\")\n",
    "print(f\"\\n  Prediction statistics:\")\n",
    "print(f\"    Mean: {submission['predicted_risk_score'].mean():.2f}\")\n",
    "print(f\"    Median: {submission['predicted_risk_score'].median():.2f}\")\n",
    "print(f\"    Std: {submission['predicted_risk_score'].std():.2f}\")\n",
    "print(f\"    Min: {submission['predicted_risk_score'].min():.2f}\")\n",
    "print(f\"    Max: {submission['predicted_risk_score'].max():.2f}\")\n",
    "\n",
    "# Visualization: Final Predictions\n",
    "print(\"\\n  Creating final predictions visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 11: Final Predictions Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prediction distribution\n",
    "axes[0, 0].hist(submission['predicted_risk_score'], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(submission['predicted_risk_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {submission[\"predicted_risk_score\"].mean():.2f}', linewidth=2)\n",
    "axes[0, 0].axvline(submission['predicted_risk_score'].median(), color='blue', linestyle='--', \n",
    "                   label=f'Median: {submission[\"predicted_risk_score\"].median():.2f}', linewidth=2)\n",
    "axes[0, 0].set_title('Predicted Risk Score Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Predicted Risk Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison with training distribution\n",
    "axes[0, 1].hist(train_risk['risk_score'], bins=50, alpha=0.5, label='Training Data', \n",
    "                color='steelblue', edgecolor='black')\n",
    "axes[0, 1].hist(submission['predicted_risk_score'], bins=50, alpha=0.5, label='Predictions', \n",
    "                color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Training vs Prediction Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Risk Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "box_data = [train_risk['risk_score'], submission['predicted_risk_score']]\n",
    "bp = axes[1, 0].boxplot(box_data, labels=['Training', 'Predictions'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "bp['boxes'][1].set_facecolor('coral')\n",
    "axes[1, 0].set_title('Risk Score Distribution: Training vs Predictions', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Risk Score')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Statistics comparison\n",
    "stats_comparison = pd.DataFrame({\n",
    "    'Training': [\n",
    "        train_risk['risk_score'].mean(),\n",
    "        train_risk['risk_score'].median(),\n",
    "        train_risk['risk_score'].std(),\n",
    "        train_risk['risk_score'].min(),\n",
    "        train_risk['risk_score'].max()\n",
    "    ],\n",
    "    'Predictions': [\n",
    "        submission['predicted_risk_score'].mean(),\n",
    "        submission['predicted_risk_score'].median(),\n",
    "        submission['predicted_risk_score'].std(),\n",
    "        submission['predicted_risk_score'].min(),\n",
    "        submission['predicted_risk_score'].max()\n",
    "    ]\n",
    "}, index=['Mean', 'Median', 'Std', 'Min', 'Max'])\n",
    "\n",
    "x = np.arange(len(stats_comparison.index))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, stats_comparison['Training'], width, label='Training', color='steelblue', alpha=0.7)\n",
    "axes[1, 1].bar(x + width/2, stats_comparison['Predictions'], width, label='Predictions', color='coral', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Statistic')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].set_title('Statistical Comparison', fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(stats_comparison.index)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '11_final_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/11_final_predictions.png\")\n",
    "\n",
    "# Save models\n",
    "for name, model in models_optimized.items():\n",
    "    if name != 'stacked':\n",
    "        with open(MODELS_DIR / f'{name}_advanced.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    else:\n",
    "        with open(MODELS_DIR / 'stacked_advanced.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "print(\"\\n✓ Models saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Model: {best_model_name.upper()}\")\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Predictions: {submission_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb72c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
