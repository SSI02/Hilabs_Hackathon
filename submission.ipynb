{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0626e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "# Hyperparameter optimization\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not available - install with: pip install optuna\")\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "TRAIN_DIR = BASE_DIR / 'pcms_hackathon_data' / 'train'\n",
    "TEST_DIR = BASE_DIR / 'pcms_hackathon_data' / 'test'\n",
    "OUTPUT_DIR = BASE_DIR / 'analysis_outputs'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED MODEL IMPROVEMENTS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n1. Loading and preparing data...\")\n",
    "\n",
    "# Load data\n",
    "train_patient = pd.read_csv(TRAIN_DIR / 'patient.csv')\n",
    "train_risk = pd.read_csv(TRAIN_DIR / 'risk.csv')\n",
    "train_visit = pd.read_csv(TRAIN_DIR / 'visit.csv')\n",
    "train_diagnosis = pd.read_csv(TRAIN_DIR / 'diagnosis.csv')\n",
    "train_care = pd.read_csv(TRAIN_DIR / 'care.csv')\n",
    "\n",
    "test_patient = pd.read_csv(TEST_DIR / 'patient.csv')\n",
    "test_visit = pd.read_csv(TEST_DIR / 'visit.csv')\n",
    "test_diagnosis = pd.read_csv(TEST_DIR / 'diagnosis.csv')\n",
    "test_care = pd.read_csv(TEST_DIR / 'care.csv')\n",
    "\n",
    "# Use the improved feature engineering from 03_improved_model_training.py\n",
    "# (Import or copy the engineer_focused_features function)\n",
    "# For now, let's create a simplified version here\n",
    "\n",
    "print(\"✓ Data loaded\")\n",
    "\n",
    "# Visualization: Data Overview\n",
    "print(\"\\n  Creating data overview visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 1: Data Loading - Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Data shapes\n",
    "data_shapes = {\n",
    "    'patient': len(train_patient),\n",
    "    'visit': len(train_visit),\n",
    "    'diagnosis': len(train_diagnosis),\n",
    "    'care': len(train_care),\n",
    "    'risk': len(train_risk)\n",
    "}\n",
    "axes[0, 0].bar(data_shapes.keys(), data_shapes.values(), color='steelblue')\n",
    "axes[0, 0].set_title('Training Data Record Counts', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Records')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(data_shapes.values()):\n",
    "    axes[0, 0].text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Missing values\n",
    "train_data_list = [train_patient, train_visit, train_diagnosis, train_care, train_risk]\n",
    "train_data_names = ['patient', 'visit', 'diagnosis', 'care', 'risk']\n",
    "missing_counts = [df.isnull().sum().sum() for df in train_data_list]\n",
    "axes[0, 1].bar(train_data_names, missing_counts, color='coral')\n",
    "axes[0, 1].set_title('Missing Values Count', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Missing Values')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Age distribution\n",
    "axes[1, 0].hist(train_patient['age'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
    "axes[1, 0].set_title('Age Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Risk score distribution\n",
    "axes[1, 1].hist(train_risk['risk_score'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[1, 1].set_title('Risk Score Distribution (Target)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Risk Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '01_data_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/01_data_overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cdea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Analyzing target distribution for transformation...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_risk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. Analyzing target distribution for transformation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Check target skewness\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m target_skew \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_risk\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mskew()\n\u001b[0;32m      5\u001b[0m target_kurt \u001b[38;5;241m=\u001b[39m train_risk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkurtosis()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Target Skewness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_skew\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_risk' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Analyzing target distribution for transformation...\")\n",
    "\n",
    "# Check target skewness\n",
    "target_skew = train_risk['risk_score'].skew()\n",
    "target_kurt = train_risk['risk_score'].kurtosis()\n",
    "\n",
    "print(f\"  Target Skewness: {target_skew:.2f}\")\n",
    "print(f\"  Target Kurtosis: {target_kurt:.2f}\")\n",
    "\n",
    "# Try different transformations\n",
    "transformations = {\n",
    "    'original': lambda x: x,\n",
    "    'log1p': lambda x: np.log1p(x),\n",
    "    'sqrt': lambda x: np.sqrt(x),\n",
    "    'box_cox': None  # Will use PowerTransformer\n",
    "}\n",
    "\n",
    "best_transformation = 'original'\n",
    "best_score = float('inf')\n",
    "\n",
    "# Test transformations with simple model\n",
    "X_sample = train_patient[['age']].fillna(train_patient['age'].median())\n",
    "y_sample = train_risk['risk_score']\n",
    "\n",
    "for trans_name, trans_func in transformations.items():\n",
    "    if trans_func is None:\n",
    "        continue\n",
    "    try:\n",
    "        y_transformed = trans_func(y_sample)\n",
    "        # Simple correlation test\n",
    "        corr = abs(X_sample['age'].corr(y_transformed))\n",
    "        if corr > 0.1:  # Some correlation exists\n",
    "            print(f\"  {trans_name}: Correlation = {corr:.3f}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"✓ Target analysis complete\")\n",
    "\n",
    "# Visualization: Target Distribution Analysis\n",
    "print(\"\\n  Creating target transformation visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 2: Target Transformation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original distribution\n",
    "axes[0, 0].hist(y_sample, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(y_sample.mean(), color='red', linestyle='--', label=f'Mean: {y_sample.mean():.2f}')\n",
    "axes[0, 0].axvline(y_sample.median(), color='green', linestyle='--', label=f'Median: {y_sample.median():.2f}')\n",
    "axes[0, 0].set_title(f'Original Distribution\\n(Skewness: {target_skew:.2f}, Kurtosis: {target_kurt:.2f})', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Risk Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-transformed distribution\n",
    "y_log = np.log1p(y_sample)\n",
    "log_skew = y_log.skew()\n",
    "log_kurt = y_log.kurtosis()\n",
    "axes[0, 1].hist(y_log, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(y_log.mean(), color='red', linestyle='--', label=f'Mean: {y_log.mean():.2f}')\n",
    "axes[0, 1].axvline(y_log.median(), color='green', linestyle='--', label=f'Median: {y_log.median():.2f}')\n",
    "axes[0, 1].set_title(f'Log1p Transformed Distribution\\n(Skewness: {log_skew:.2f}, Kurtosis: {log_kurt:.2f})', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('log1p(Risk Score)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot original\n",
    "stats.probplot(y_sample, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot: Original Distribution', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot transformed\n",
    "stats.probplot(y_log, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot: Log-Transformed Distribution', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '02_target_transformation.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/02_target_transformation.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9794c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Outlier detection and handling...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_risk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outliers\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Detect outliers in target\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m target_outliers \u001b[38;5;241m=\u001b[39m detect_outliers_iqr(\u001b[43mtrain_risk\u001b[49m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Risk score outliers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_outliers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_outliers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Strategy: Cap extreme outliers in features, but keep in target (they're real high-risk patients)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_risk' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. Outlier detection and handling...\")\n",
    "\n",
    "def detect_outliers_iqr(data, columns):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    outliers = {}\n",
    "    for col in columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outlier_count = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()\n",
    "        outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percentage': (outlier_count / len(data)) * 100,\n",
    "            'bounds': (lower_bound, upper_bound)\n",
    "        }\n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in target\n",
    "target_outliers = detect_outliers_iqr(train_risk, ['risk_score'])\n",
    "print(f\"  Risk score outliers: {target_outliers['risk_score']['count']} ({target_outliers['risk_score']['percentage']:.2f}%)\")\n",
    "\n",
    "# Strategy: Cap extreme outliers in features, but keep in target (they're real high-risk patients)\n",
    "print(\"✓ Outlier analysis complete\")\n",
    "\n",
    "# Visualization: Outlier Analysis\n",
    "print(\"\\n  Creating outlier analysis visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Step 3: Outlier Detection and Handling', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Box plot for risk score\n",
    "risk_data = train_risk['risk_score']\n",
    "Q1 = risk_data.quantile(0.25)\n",
    "Q3 = risk_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "axes[0].boxplot(risk_data, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "axes[0].axhline(lower_bound, color='red', linestyle='--', label=f'Lower bound: {lower_bound:.2f}')\n",
    "axes[0].axhline(upper_bound, color='red', linestyle='--', label=f'Upper bound: {upper_bound:.2f}')\n",
    "axes[0].set_title(f'Risk Score Outliers\\n(Outliers: {target_outliers[\"risk_score\"][\"count\"]}, {target_outliers[\"risk_score\"][\"percentage\"]:.2f}%)', \n",
    "                  fontweight='bold')\n",
    "axes[0].set_ylabel('Risk Score')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution with outlier region highlighted\n",
    "axes[1].hist(risk_data, bins=50, color='steelblue', edgecolor='black', alpha=0.7, label='All Data')\n",
    "outliers = risk_data[(risk_data < lower_bound) | (risk_data > upper_bound)]\n",
    "if len(outliers) > 0:\n",
    "    axes[1].hist(outliers, bins=20, color='red', edgecolor='black', alpha=0.7, label='Outliers')\n",
    "axes[1].axvline(lower_bound, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].axvline(upper_bound, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].fill_betweenx([0, axes[1].get_ylim()[1]], lower_bound, upper_bound, \n",
    "                       alpha=0.2, color='green', label='Normal Range')\n",
    "axes[1].set_title('Risk Score Distribution with Outlier Bounds', fontweight='bold')\n",
    "axes[1].set_xlabel('Risk Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '03_outlier_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/03_outlier_analysis.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542bbcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Advanced feature engineering...\n",
      "  Engineering features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_patient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 156\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Engineer features\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Engineering features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m train_features \u001b[38;5;241m=\u001b[39m engineer_advanced_features(\u001b[43mtrain_patient\u001b[49m, train_visit, train_diagnosis, train_care)\n\u001b[0;32m    157\u001b[0m test_features \u001b[38;5;241m=\u001b[39m engineer_advanced_features(test_patient, test_visit, test_diagnosis, test_care)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Features engineered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_patient' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Advanced feature engineering...\")\n",
    "\n",
    "def engineer_advanced_features(patient, visit, diagnosis, care):\n",
    "    \"\"\"Advanced feature engineering with domain insights\"\"\"\n",
    "    \n",
    "    features = patient[['patient_id', 'age']].copy()\n",
    "    \n",
    "    # Age bins (more granular)\n",
    "    features['age_bin_18_35'] = ((features['age'] >= 18) & (features['age'] <= 35)).astype(int)\n",
    "    features['age_bin_36_50'] = ((features['age'] >= 36) & (features['age'] <= 50)).astype(int)\n",
    "    features['age_bin_51_65'] = ((features['age'] >= 51) & (features['age'] <= 65)).astype(int)\n",
    "    features['age_bin_65_plus'] = (features['age'] > 65).astype(int)\n",
    "    \n",
    "    # Hot spotter\n",
    "    features['is_hot_spotter'] = (patient['hot_spotter_identified_at'] != '0001-01-01').astype(int)\n",
    "    features['hot_spotter_readmission_flag'] = (patient['hot_spotter_readmission_flag'] == 't').astype(int)\n",
    "    features['hot_spotter_chronic_flag'] = (patient['hot_spotter_chronic_flag'] == 't').astype(int)\n",
    "    \n",
    "    # ===== VISIT FEATURES (Enhanced) =====\n",
    "    visit_agg = visit.groupby('patient_id').agg({\n",
    "        'visit_id': 'count',\n",
    "        'readmsn_ind': lambda x: (x == 't').sum(),\n",
    "    }).reset_index()\n",
    "    visit_agg.columns = ['patient_id', 'total_visits', 'readmission_count']\n",
    "    \n",
    "    # Visit type counts\n",
    "    visit_type_counts = visit.groupby(['patient_id', 'visit_type']).size().unstack(fill_value=0)\n",
    "    visit_type_cols = []\n",
    "    for col in visit_type_counts.columns:\n",
    "        col_name = f'visits_{col.lower().replace(\" \", \"_\")}'\n",
    "        visit_type_counts[col_name] = visit_type_counts[col]\n",
    "        visit_type_cols.append(col_name)\n",
    "    visit_type_counts = visit_type_counts[visit_type_cols]\n",
    "    \n",
    "    # Visit duration features\n",
    "    visit['visit_start_dt'] = pd.to_datetime(visit['visit_start_dt'], errors='coerce')\n",
    "    visit['visit_end_dt'] = pd.to_datetime(visit['visit_end_dt'], errors='coerce')\n",
    "    visit['visit_duration'] = (visit['visit_end_dt'] - visit['visit_start_dt']).dt.days.fillna(0).clip(0, 365)\n",
    "    \n",
    "    visit_duration_agg = visit.groupby('patient_id')['visit_duration'].agg(['mean', 'max', 'min', 'std', 'sum']).reset_index()\n",
    "    visit_duration_agg.columns = ['patient_id', 'avg_visit_duration', 'max_visit_duration', \n",
    "                                  'min_visit_duration', 'std_visit_duration', 'total_visit_days']\n",
    "    \n",
    "    # Visit frequency patterns\n",
    "    visit['visit_start_dt'] = pd.to_datetime(visit['visit_start_dt'], errors='coerce')\n",
    "    REFERENCE_DATE = pd.to_datetime('2025-03-01')\n",
    "    visit['days_since_visit'] = (REFERENCE_DATE - visit['visit_start_dt']).dt.days.fillna(365).clip(0, 365*5)\n",
    "    \n",
    "    # Recent visits\n",
    "    for days in [30, 60, 90]:\n",
    "        recent = visit[visit['days_since_visit'] <= days].groupby('patient_id').size()\n",
    "        visit_agg[f'visits_last_{days}_days'] = visit_agg['patient_id'].map(recent).fillna(0)\n",
    "    \n",
    "    # Emergency visits\n",
    "    visit['is_emergency'] = ((visit['visit_type'] == 'ER') | (visit['visit_type'] == 'INPATIENT')).astype(int)\n",
    "    emergency_agg = visit.groupby('patient_id')['is_emergency'].agg(['sum', 'mean']).reset_index()\n",
    "    emergency_agg.columns = ['patient_id', 'emergency_visit_count', 'emergency_visit_rate']\n",
    "    \n",
    "    features = features.merge(visit_agg, on='patient_id', how='left')\n",
    "    features = features.merge(visit_type_counts, on='patient_id', how='left')\n",
    "    features = features.merge(visit_duration_agg, on='patient_id', how='left')\n",
    "    features = features.merge(emergency_agg, on='patient_id', how='left')\n",
    "    \n",
    "    # Visit ratios\n",
    "    if 'total_visits' in features.columns:\n",
    "        for col in visit_type_cols:\n",
    "            if col in features.columns:\n",
    "                features[f'{col}_ratio'] = features[col] / (features['total_visits'] + 1)\n",
    "    \n",
    "    # ===== DIAGNOSIS FEATURES =====\n",
    "    diagnosis['is_chronic'] = (diagnosis['is_chronic'] == 't').astype(int) if diagnosis['is_chronic'].dtype == 'object' else diagnosis['is_chronic'].astype(int)\n",
    "    chronic_conditions = diagnosis[diagnosis['is_chronic'] == 1]\n",
    "    \n",
    "    diagnosis_agg = diagnosis.groupby('patient_id').agg({\n",
    "        'diagnosis_id': 'count',\n",
    "        'is_chronic': 'sum',\n",
    "    }).reset_index()\n",
    "    diagnosis_agg.columns = ['patient_id', 'total_diagnoses', 'chronic_count']\n",
    "    \n",
    "    # Specific conditions\n",
    "    for condition in ['CANCER', 'DIABETES', 'HYPERTENSION']:\n",
    "        condition_patients = chronic_conditions[chronic_conditions['condition_name'] == condition]['patient_id'].unique()\n",
    "        diagnosis_agg[f'has_{condition.lower()}'] = diagnosis_agg['patient_id'].isin(condition_patients).astype(int)\n",
    "    \n",
    "    # Severity score\n",
    "    diagnosis_agg['chronic_severity_score'] = (\n",
    "        diagnosis_agg['has_cancer'] * 5 +\n",
    "        diagnosis_agg['has_diabetes'] * 2 +\n",
    "        diagnosis_agg['has_hypertension'] * 1\n",
    "    )\n",
    "    \n",
    "    features = features.merge(diagnosis_agg, on='patient_id', how='left')\n",
    "    \n",
    "    # ===== CARE FEATURES =====\n",
    "    care['care_gap_ind'] = (care['care_gap_ind'] == 't').astype(int) if care['care_gap_ind'].dtype == 'object' else care['care_gap_ind'].astype(int)\n",
    "    \n",
    "    care_agg = care.groupby('patient_id').agg({\n",
    "        'care_id': 'count',\n",
    "        'care_gap_ind': 'sum',\n",
    "    }).reset_index()\n",
    "    care_agg.columns = ['patient_id', 'total_care_records', 'care_gaps']\n",
    "    care_agg['care_gap_rate'] = care_agg['care_gaps'] / (care_agg['total_care_records'] + 1)\n",
    "    \n",
    "    features = features.merge(care_agg, on='patient_id', how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'patient_id':\n",
    "            features[col] = features[col].fillna(0)\n",
    "    \n",
    "    # ===== ADVANCED INTERACTIONS =====\n",
    "    \n",
    "    # High-risk combinations\n",
    "    if 'has_cancer' in features.columns and 'visits_inpatient' in features.columns:\n",
    "        features['cancer_x_inpatient'] = features['has_cancer'] * features['visits_inpatient']\n",
    "    \n",
    "    if 'has_cancer' in features.columns and 'readmission_count' in features.columns:\n",
    "        features['cancer_x_readmission'] = features['has_cancer'] * features['readmission_count']\n",
    "    \n",
    "    if 'chronic_severity_score' in features.columns and 'emergency_visit_count' in features.columns:\n",
    "        features['chronic_x_emergency'] = features['chronic_severity_score'] * features['emergency_visit_count']\n",
    "    \n",
    "    if 'care_gap_rate' in features.columns and 'has_cancer' in features.columns:\n",
    "        features['gaps_x_cancer'] = features['care_gap_rate'] * features['has_cancer'] * 10\n",
    "    \n",
    "    # Age risk interactions\n",
    "    if 'age' in features.columns:\n",
    "        if 'has_cancer' in features.columns:\n",
    "            features['elderly_cancer'] = ((features['age'] > 65) & (features['has_cancer'] == 1)).astype(int)\n",
    "        if 'emergency_visit_count' in features.columns:\n",
    "            features['elderly_emergency'] = ((features['age'] > 65) & (features['emergency_visit_count'] > 0)).astype(int)\n",
    "    \n",
    "    # Composite high-risk flags\n",
    "    features['high_risk_flag'] = (\n",
    "        (features.get('has_cancer', 0) == 1) |\n",
    "        (features.get('visits_inpatient', 0) > 2) |\n",
    "        (features.get('readmission_count', 0) > 1) |\n",
    "        (features.get('is_hot_spotter', 0) == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Risk score components\n",
    "    features['visit_risk_component'] = (\n",
    "        features.get('emergency_visit_count', 0) * 2 +\n",
    "        features.get('readmission_count', 0) * 3\n",
    "    )\n",
    "    \n",
    "    features['chronic_risk_component'] = features.get('chronic_severity_score', 0) * 2\n",
    "    \n",
    "    features['care_risk_component'] = features.get('care_gap_rate', 0) * 10\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Engineer features\n",
    "print(\"  Engineering features...\")\n",
    "train_features = engineer_advanced_features(train_patient, train_visit, train_diagnosis, train_care)\n",
    "test_features = engineer_advanced_features(test_patient, test_visit, test_diagnosis, test_care)\n",
    "\n",
    "print(f\"✓ Features engineered: {train_features.shape[1]} features\")\n",
    "\n",
    "# Visualization: Feature Engineering Summary\n",
    "print(\"\\n  Creating feature engineering visualizations...\")\n",
    "feature_categories = {\n",
    "    'Demographics': ['age', 'age_bin_18_35', 'age_bin_36_50', 'age_bin_51_65', 'age_bin_65_plus'],\n",
    "    'Hot Spotter': ['is_hot_spotter', 'hot_spotter_readmission_flag', 'hot_spotter_chronic_flag'],\n",
    "    'Visits': [col for col in train_features.columns if 'visit' in col.lower() and col not in ['patient_id']],\n",
    "    'Diagnosis': [col for col in train_features.columns if 'diagnosis' in col.lower() or 'chronic' in col.lower() or 'cancer' in col.lower() or 'diabetes' in col.lower() or 'hypertension' in col.lower()],\n",
    "    'Care': [col for col in train_features.columns if 'care' in col.lower() or 'gap' in col.lower()],\n",
    "    'Interactions': [col for col in train_features.columns if 'x_' in col or 'elderly' in col.lower() or 'risk_component' in col.lower() or 'high_risk' in col.lower()]\n",
    "}\n",
    "category_counts = {cat: len([f for f in feats if f in train_features.columns]) for cat, feats in feature_categories.items()}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Step 4: Feature Engineering Summary', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Feature counts by category\n",
    "axes[0].bar(category_counts.keys(), category_counts.values(), color='steelblue')\n",
    "axes[0].set_title('Features by Category', fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Features')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, (cat, count) in enumerate(category_counts.items()):\n",
    "    axes[0].text(i, count, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Top 15 feature correlation with target (if available)\n",
    "train_data_viz = train_features.merge(train_risk[['patient_id', 'risk_score']], on='patient_id', how='inner')\n",
    "if 'risk_score' in train_data_viz.columns:\n",
    "    numeric_features = train_data_viz.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'patient_id' in numeric_features:\n",
    "        numeric_features.remove('patient_id')\n",
    "    if 'risk_score' in numeric_features:\n",
    "        numeric_features.remove('risk_score')\n",
    "    \n",
    "    if len(numeric_features) > 0:\n",
    "        correlations = train_data_viz[numeric_features + ['risk_score']].corr()['risk_score'].abs().sort_values(ascending=False)\n",
    "        top_features = correlations.head(15).index.tolist()\n",
    "        if 'risk_score' in top_features:\n",
    "            top_features.remove('risk_score')\n",
    "        top_corrs = correlations[top_features].sort_values(ascending=True)\n",
    "        \n",
    "        axes[1].barh(range(len(top_corrs)), top_corrs.values, color='coral')\n",
    "        axes[1].set_yticks(range(len(top_corrs)))\n",
    "        axes[1].set_yticklabels([f[:30] for f in top_corrs.index])  # Truncate long names\n",
    "        axes[1].set_title('Top 15 Feature Correlations with Risk Score', fontweight='bold')\n",
    "        axes[1].set_xlabel('Absolute Correlation')\n",
    "        axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '04_feature_engineering.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/04_feature_engineering.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4545937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Preparing data with transformations...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5. Preparing data with transformations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_features\u001b[49m\u001b[38;5;241m.\u001b[39mmerge(train_risk[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m train_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_score\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m train_data[feature_cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Preparing data with transformations...\")\n",
    "\n",
    "train_data = train_features.merge(train_risk[['patient_id', 'risk_score']], on='patient_id', how='inner')\n",
    "\n",
    "feature_cols = [col for col in train_data.columns if col not in ['patient_id', 'risk_score']]\n",
    "\n",
    "X = train_data[feature_cols].fillna(0)\n",
    "y = train_data['risk_score']\n",
    "\n",
    "# Handle outliers in features (cap extreme values)\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in [np.int64, np.float64]:\n",
    "        Q1, Q3 = X[col].quantile([0.01, 0.99])\n",
    "        X[col] = X[col].clip(lower=Q1, upper=Q3)\n",
    "\n",
    "# Replace inf and large values\n",
    "X = X.replace([np.inf, -np.inf], 0)\n",
    "X = X.clip(lower=-1000, upper=1000)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"✓ Training set: {X_train.shape}\")\n",
    "print(f\"✓ Validation set: {X_val.shape}\")\n",
    "\n",
    "# Try log transformation for target\n",
    "print(\"\\n  Testing target transformations...\")\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "# Compare original vs log-transformed\n",
    "orig_skew = y_train.skew()\n",
    "log_skew = y_train_log.skew()\n",
    "\n",
    "print(f\"  Original skewness: {orig_skew:.2f}\")\n",
    "print(f\"  Log-transformed skewness: {log_skew:.2f}\")\n",
    "\n",
    "use_log_transform = abs(log_skew) < abs(orig_skew)\n",
    "print(f\"  Will use log transformation: {use_log_transform}\")\n",
    "\n",
    "if use_log_transform:\n",
    "    y_train_model = y_train_log\n",
    "    y_val_model = y_val_log\n",
    "else:\n",
    "    y_train_model = y_train\n",
    "    y_val_model = y_val\n",
    "\n",
    "# Visualization: Data Preparation\n",
    "print(\"\\n  Creating data preparation visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 5: Data Preparation and Transformation', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Train/Val split visualization\n",
    "split_data = {'Train': len(y_train), 'Validation': len(y_val)}\n",
    "axes[0, 0].pie(split_data.values(), labels=split_data.keys(), autopct='%1.1f%%', \n",
    "               colors=['steelblue', 'coral'], startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[0, 0].set_title(f'Train/Validation Split\\n(Total: {len(y_train) + len(y_val)} samples)', fontweight='bold')\n",
    "\n",
    "# Target distribution comparison\n",
    "axes[0, 1].hist(y_train, bins=30, alpha=0.7, label='Train', color='steelblue', edgecolor='black')\n",
    "axes[0, 1].hist(y_val, bins=30, alpha=0.7, label='Validation', color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Target Distribution: Train vs Validation', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Risk Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Transformation comparison\n",
    "axes[1, 0].hist(y_train_model if use_log_transform else y_train, bins=30, \n",
    "                color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "transform_type = 'log1p' if use_log_transform else 'original'\n",
    "axes[1, 0].set_title(f'Transformed Target Distribution\\n(Using: {transform_type})', fontweight='bold')\n",
    "axes[1, 0].set_xlabel(f'Risk Score ({transform_type})')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature count by type\n",
    "numeric_features_count = X.select_dtypes(include=[np.number]).shape[1]\n",
    "axes[1, 1].bar(['Numeric Features'], [numeric_features_count], color='skyblue', width=0.5)\n",
    "axes[1, 1].text(0, numeric_features_count, str(numeric_features_count), \n",
    "               ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_title(f'Final Feature Set\\n({numeric_features_count} features)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '05_data_preparation.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/05_data_preparation.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d24378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Hyperparameter optimization...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OPTUNA_AVAILABLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m6. Hyperparameter optimization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mOPTUNA_AVAILABLE\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Using Optuna for hyperparameter tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_xgb\u001b[39m(trial):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OPTUNA_AVAILABLE' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. Hyperparameter optimization...\")\n",
    "\n",
    "if OPTUNA_AVAILABLE:\n",
    "    print(\"  Using Optuna for hyperparameter tuning...\")\n",
    "    \n",
    "    def objective_xgb(trial):\n",
    "        \"\"\"XGBoost objective function\"\"\"\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 1500),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 2.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 2.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        scores = cross_val_score(model, X_train, y_train_model, \n",
    "                                cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                n_jobs=-1)\n",
    "        return -scores.mean()\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"  Optimizing XGBoost...\")\n",
    "    study_xgb = optuna.create_study(direction='minimize', study_name='xgb_optimization')\n",
    "    study_xgb.optimize(objective_xgb, n_trials=20, show_progress_bar=True)\n",
    "    \n",
    "    best_xgb_params = study_xgb.best_params\n",
    "    best_xgb_params['objective'] = 'reg:squarederror'\n",
    "    best_xgb_params['eval_metric'] = 'rmse'\n",
    "    best_xgb_params['random_state'] = 42\n",
    "    best_xgb_params['verbosity'] = 0\n",
    "    \n",
    "    print(f\"  ✓ Best XGBoost RMSE: {study_xgb.best_value:.4f}\")\n",
    "    print(f\"  ✓ Best params: {best_xgb_params}\")\n",
    "    \n",
    "    # Visualization: Optuna Optimization History\n",
    "    print(\"\\n  Creating hyperparameter optimization visualizations...\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Step 6: Hyperparameter Optimization (Optuna)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Optimization history\n",
    "    opt_history = study_xgb.trials_dataframe()\n",
    "    axes[0].plot(range(len(opt_history)), opt_history['value'], marker='o', \n",
    "                 color='steelblue', linewidth=2, markersize=4)\n",
    "    axes[0].axhline(study_xgb.best_value, color='red', linestyle='--', \n",
    "                   label=f'Best RMSE: {study_xgb.best_value:.4f}', linewidth=2)\n",
    "    axes[0].set_title('Optimization History', fontweight='bold')\n",
    "    axes[0].set_xlabel('Trial Number')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter importance (if available)\n",
    "    try:\n",
    "        param_importance = optuna.importance.get_param_importances(study_xgb)\n",
    "        if param_importance:\n",
    "            params = list(param_importance.keys())\n",
    "            importances = list(param_importance.values())\n",
    "            axes[1].barh(range(len(params)), importances, color='coral')\n",
    "            axes[1].set_yticks(range(len(params)))\n",
    "            axes[1].set_yticklabels(params)\n",
    "            axes[1].set_title('Parameter Importance', fontweight='bold')\n",
    "            axes[1].set_xlabel('Importance')\n",
    "            axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    except:\n",
    "        axes[1].text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes,\n",
    "                    fontsize=12, style='italic')\n",
    "        axes[1].set_title('Parameter Importance', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / '06_hyperparameter_optimization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  ✓ Saved: analysis_outputs/06_hyperparameter_optimization.png\")\n",
    "else:\n",
    "    print(\"  Optuna not available, using optimized defaults...\")\n",
    "    best_xgb_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 7,\n",
    "        'learning_rate': 0.03,\n",
    "        'n_estimators': 1000,\n",
    "        'subsample': 0.85,\n",
    "        'colsample_bytree': 0.85,\n",
    "        'min_child_weight': 2,\n",
    "        'reg_alpha': 0.2,\n",
    "        'reg_lambda': 2,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c2e1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Training optimized models...\n",
      "  Training XGBoost...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# XGBoost (optimized)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training XGBoost...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      9\u001b[0m xgb_opt \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_xgb_params)\n\u001b[0;32m     10\u001b[0m xgb_opt\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_model,\n\u001b[0;32m     11\u001b[0m             eval_set\u001b[38;5;241m=\u001b[39m[(X_train, y_train_model), (X_val, y_val_model)],\n\u001b[0;32m     12\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n7. Training optimized models...\")\n",
    "\n",
    "models_optimized = {}\n",
    "predictions_val_opt = {}\n",
    "\n",
    "# XGBoost (optimized)\n",
    "print(\"  Training XGBoost...\")\n",
    "start = time.time()\n",
    "xgb_opt = xgb.XGBRegressor(**best_xgb_params)\n",
    "xgb_opt.fit(X_train, y_train_model,\n",
    "            eval_set=[(X_train, y_train_model), (X_val, y_val_model)],\n",
    "            verbose=False)\n",
    "models_optimized['xgb'] = xgb_opt\n",
    "\n",
    "pred_val_xgb = xgb_opt.predict(X_val)\n",
    "if use_log_transform:\n",
    "    pred_val_xgb = np.expm1(pred_val_xgb)\n",
    "predictions_val_opt['xgb'] = pred_val_xgb\n",
    "\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, pred_val_xgb))\n",
    "print(f\"    ✓ XGBoost RMSE: {xgb_rmse:.4f} (trained in {time.time()-start:.2f}s)\")\n",
    "\n",
    "# LightGBM (optimized)\n",
    "print(\"  Training LightGBM...\")\n",
    "start = time.time()\n",
    "lgb_opt = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    max_depth=7,\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=1000,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    min_child_samples=2,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=2,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "lgb_opt.fit(X_train, y_train_model,\n",
    "           eval_set=[(X_train, y_train_model), (X_val, y_val_model)],\n",
    "           callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                     lgb.log_evaluation(period=0)])\n",
    "models_optimized['lgb'] = lgb_opt\n",
    "\n",
    "pred_val_lgb = lgb_opt.predict(X_val)\n",
    "if use_log_transform:\n",
    "    pred_val_lgb = np.expm1(pred_val_lgb)\n",
    "predictions_val_opt['lgb'] = pred_val_lgb\n",
    "\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_val, pred_val_lgb))\n",
    "print(f\"    ✓ LightGBM RMSE: {lgb_rmse:.4f} (trained in {time.time()-start:.2f}s)\")\n",
    "\n",
    "# Visualization: Model Training Results\n",
    "print(\"\\n  Creating model training visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 7: Model Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Predictions vs Actual - XGBoost\n",
    "axes[0, 0].scatter(y_val, pred_val_xgb, alpha=0.5, color='steelblue', s=20)\n",
    "axes[0, 0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_title(f'XGBoost: Predictions vs Actual\\n(RMSE: {xgb_rmse:.4f})', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Actual Risk Score')\n",
    "axes[0, 0].set_ylabel('Predicted Risk Score')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual - LightGBM\n",
    "axes[0, 1].scatter(y_val, pred_val_lgb, alpha=0.5, color='coral', s=20)\n",
    "axes[0, 1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_title(f'LightGBM: Predictions vs Actual\\n(RMSE: {lgb_rmse:.4f})', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Actual Risk Score')\n",
    "axes[0, 1].set_ylabel('Predicted Risk Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals - XGBoost\n",
    "residuals_xgb = y_val - pred_val_xgb\n",
    "axes[1, 0].scatter(pred_val_xgb, residuals_xgb, alpha=0.5, color='steelblue', s=20)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_title('XGBoost: Residuals Plot', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Predicted Risk Score')\n",
    "axes[1, 0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals - LightGBM\n",
    "residuals_lgb = y_val - pred_val_lgb\n",
    "axes[1, 1].scatter(pred_val_lgb, residuals_lgb, alpha=0.5, color='coral', s=20)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_title('LightGBM: Residuals Plot', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Predicted Risk Score')\n",
    "axes[1, 1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '07_model_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/07_model_training.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37dfe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Creating stacking model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8. Creating stacking model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use predictions from base models as features for meta-model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_val_stacked \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions_val_opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions_val_opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m })\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train meta-model (XGBoost)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m meta_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\n\u001b[0;32m     11\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     12\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n8. Creating stacking model...\")\n",
    "\n",
    "# Use predictions from base models as features for meta-model\n",
    "X_val_stacked = pd.DataFrame({\n",
    "    'xgb_pred': predictions_val_opt['xgb'],\n",
    "    'lgb_pred': predictions_val_opt['lgb'],\n",
    "})\n",
    "\n",
    "# Train meta-model (XGBoost)\n",
    "meta_model = xgb.XGBRegressor(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Get train predictions for stacking\n",
    "X_train_stacked = pd.DataFrame({\n",
    "    'xgb_pred': xgb_opt.predict(X_train),\n",
    "    'lgb_pred': lgb_opt.predict(X_train),\n",
    "})\n",
    "\n",
    "if use_log_transform:\n",
    "    X_train_stacked['xgb_pred'] = np.expm1(X_train_stacked['xgb_pred'])\n",
    "    X_train_stacked['lgb_pred'] = np.expm1(X_train_stacked['lgb_pred'])\n",
    "\n",
    "meta_model.fit(X_train_stacked, y_train)\n",
    "\n",
    "# Stacking predictions\n",
    "y_val_stacked = meta_model.predict(X_val_stacked)\n",
    "stacked_rmse = np.sqrt(mean_squared_error(y_val, y_val_stacked))\n",
    "\n",
    "print(f\"  ✓ Stacking RMSE: {stacked_rmse:.4f}\")\n",
    "\n",
    "models_optimized['stacked'] = {\n",
    "    'base_models': models_optimized,\n",
    "    'meta_model': meta_model,\n",
    "    'use_log_transform': use_log_transform\n",
    "}\n",
    "\n",
    "# Visualization: Stacking Model\n",
    "print(\"\\n  Creating stacking model visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Step 8: Stacking Model', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Base model predictions correlation\n",
    "axes[0].scatter(predictions_val_opt['xgb'], predictions_val_opt['lgb'], \n",
    "                alpha=0.5, color='steelblue', s=20)\n",
    "corr_coef = np.corrcoef(predictions_val_opt['xgb'], predictions_val_opt['lgb'])[0, 1]\n",
    "axes[0].set_title(f'Base Model Predictions Correlation\\n(Correlation: {corr_coef:.3f})', fontweight='bold')\n",
    "axes[0].set_xlabel('XGBoost Predictions')\n",
    "axes[0].set_ylabel('LightGBM Predictions')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stacking predictions vs Actual\n",
    "axes[1].scatter(y_val, y_val_stacked, alpha=0.5, color='coral', s=20)\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_title(f'Stacked Model: Predictions vs Actual\\n(RMSE: {stacked_rmse:.4f})', fontweight='bold')\n",
    "axes[1].set_xlabel('Actual Risk Score')\n",
    "axes[1].set_ylabel('Predicted Risk Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '08_stacking_model.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/08_stacking_model.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2afd25e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Model Comparison:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                RMSE            MAE             R²             \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stacked_rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     r2 \u001b[38;5;241m=\u001b[39m r2_score(y_val, pred)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTACKED\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstacked_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_absolute_error(y_val,\u001b[38;5;250m \u001b[39my_val_stacked)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_score(y_val,\u001b[38;5;250m \u001b[39my_val_stacked)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Select best model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stacked_rmse' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n9. Model Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<20} {'RMSE':<15} {'MAE':<15} {'R²':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, pred in predictions_val_opt.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    print(f\"{model_name.upper():<20} {rmse:<15.4f} {mae:<15.4f} {r2:<15.4f}\")\n",
    "\n",
    "print(f\"{'STACKED':<20} {stacked_rmse:<15.4f} {mean_absolute_error(y_val, y_val_stacked):<15.4f} {r2_score(y_val, y_val_stacked):<15.4f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Select best model\n",
    "all_results = {name: np.sqrt(mean_squared_error(y_val, pred)) for name, pred in predictions_val_opt.items()}\n",
    "all_results['stacked'] = stacked_rmse\n",
    "best_model_name = min(all_results, key=all_results.get)\n",
    "best_rmse = all_results[best_model_name]\n",
    "\n",
    "print(f\"\\n✓ Best model: {best_model_name.upper()} with RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# Visualization: Model Comparison\n",
    "print(\"\\n  Creating model comparison visualizations...\")\n",
    "model_results = {}\n",
    "for model_name, pred in predictions_val_opt.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    model_results[model_name.upper()] = {'RMSE': rmse, 'MAE': mae, 'R²': r2}\n",
    "\n",
    "model_results['STACKED'] = {\n",
    "    'RMSE': stacked_rmse,\n",
    "    'MAE': mean_absolute_error(y_val, y_val_stacked),\n",
    "    'R²': r2_score(y_val, y_val_stacked)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Step 9: Model Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "models = list(model_results.keys())\n",
    "rmse_values = [model_results[m]['RMSE'] for m in models]\n",
    "mae_values = [model_results[m]['MAE'] for m in models]\n",
    "r2_values = [model_results[m]['R²'] for m in models]\n",
    "\n",
    "# RMSE comparison\n",
    "colors = ['red' if m == best_model_name.upper() else 'steelblue' for m in models]\n",
    "axes[0].bar(models, rmse_values, color=colors, alpha=0.7)\n",
    "axes[0].set_title('RMSE Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, rmse) in enumerate(zip(models, rmse_values)):\n",
    "    axes[0].text(i, rmse, f'{rmse:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(models, mae_values, color=colors, alpha=0.7)\n",
    "axes[1].set_title('MAE Comparison', fontweight='bold')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, mae) in enumerate(zip(models, mae_values)):\n",
    "    axes[1].text(i, mae, f'{mae:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# R² comparison\n",
    "axes[2].bar(models, r2_values, color=colors, alpha=0.7)\n",
    "axes[2].set_title('R² Score Comparison', fontweight='bold')\n",
    "axes[2].set_ylabel('R² Score')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, r2) in enumerate(zip(models, r2_values)):\n",
    "    axes[2].text(i, r2, f'{r2:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '09_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/09_model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217aca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Analyzing feature importance...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m10. Analyzing feature importance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get importance from XGBoost\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_cols,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m: xgb_opt\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m      7\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Top 30 Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n10. Analyzing feature importance...\")\n",
    "\n",
    "# Get importance from XGBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_opt.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n  Top 30 Features:\")\n",
    "print(\"  \" + \"-\" * 60)\n",
    "for i, row in feature_importance.head(30).iterrows():\n",
    "    print(f\"  {i+1:2d}. {row['feature']:40s} {row['importance']:>10.4f}\")\n",
    "\n",
    "# Save importance\n",
    "feature_importance.to_csv(OUTPUT_DIR / 'feature_importance_advanced.csv', index=False)\n",
    "\n",
    "# Visualization: Feature Importance\n",
    "print(\"\\n  Creating feature importance visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle('Step 10: Feature Importance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Top 20 features\n",
    "top_features = feature_importance.head(20)\n",
    "axes[0].barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\n",
    "axes[0].set_yticks(range(len(top_features)))\n",
    "axes[0].set_yticklabels([f[:40] for f in top_features['feature'].values], fontsize=9)\n",
    "axes[0].set_title('Top 20 Most Important Features', fontweight='bold')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Importance distribution\n",
    "axes[1].hist(feature_importance['importance'], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(feature_importance['importance'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {feature_importance[\"importance\"].mean():.4f}', linewidth=2)\n",
    "axes[1].axvline(feature_importance['importance'].median(), color='green', linestyle='--', \n",
    "                label=f'Median: {feature_importance[\"importance\"].median():.4f}', linewidth=2)\n",
    "axes[1].set_title('Feature Importance Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '10_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/10_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b2604aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. Generating final predictions...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m11. Generating final predictions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Prepare test features\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_features\u001b[49m[feature_cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Handle outliers and inf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X_test\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n11. Generating final predictions...\")\n",
    "\n",
    "# Prepare test features\n",
    "X_test = test_features[feature_cols].fillna(0)\n",
    "\n",
    "# Handle outliers and inf\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].dtype in [np.int64, np.float64]:\n",
    "        Q1, Q3 = X_test[col].quantile([0.01, 0.99])\n",
    "        X_test[col] = X_test[col].clip(lower=Q1, upper=Q3)\n",
    "X_test = X_test.replace([np.inf, -np.inf], 0).clip(-1000, 1000)\n",
    "\n",
    "# Generate predictions with best model\n",
    "if best_model_name == 'stacked':\n",
    "    # Stacking predictions\n",
    "    pred_xgb_test = xgb_opt.predict(X_test)\n",
    "    pred_lgb_test = lgb_opt.predict(X_test)\n",
    "    \n",
    "    if use_log_transform:\n",
    "        pred_xgb_test = np.expm1(pred_xgb_test)\n",
    "        pred_lgb_test = np.expm1(pred_lgb_test)\n",
    "    \n",
    "    X_test_stacked = pd.DataFrame({\n",
    "        'xgb_pred': pred_xgb_test,\n",
    "        'lgb_pred': pred_lgb_test,\n",
    "    })\n",
    "    test_predictions = meta_model.predict(X_test_stacked)\n",
    "else:\n",
    "    pred = models_optimized[best_model_name].predict(X_test)\n",
    "    if use_log_transform:\n",
    "        test_predictions = np.expm1(pred)\n",
    "    else:\n",
    "        test_predictions = pred\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'patient_id': test_features['patient_id'],\n",
    "    'predicted_risk_score': test_predictions\n",
    "})\n",
    "\n",
    "# Post-processing: Ensure reasonable range\n",
    "submission['predicted_risk_score'] = submission['predicted_risk_score'].clip(lower=0.1, upper=100)\n",
    "\n",
    "# Save\n",
    "submission_file = BASE_DIR / 'predictions.csv'\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Predictions saved to {submission_file}\")\n",
    "print(f\"\\n  Prediction statistics:\")\n",
    "print(f\"    Mean: {submission['predicted_risk_score'].mean():.2f}\")\n",
    "print(f\"    Median: {submission['predicted_risk_score'].median():.2f}\")\n",
    "print(f\"    Std: {submission['predicted_risk_score'].std():.2f}\")\n",
    "print(f\"    Min: {submission['predicted_risk_score'].min():.2f}\")\n",
    "print(f\"    Max: {submission['predicted_risk_score'].max():.2f}\")\n",
    "\n",
    "# Visualization: Final Predictions\n",
    "print(\"\\n  Creating final predictions visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Step 11: Final Predictions Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prediction distribution\n",
    "axes[0, 0].hist(submission['predicted_risk_score'], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(submission['predicted_risk_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {submission[\"predicted_risk_score\"].mean():.2f}', linewidth=2)\n",
    "axes[0, 0].axvline(submission['predicted_risk_score'].median(), color='blue', linestyle='--', \n",
    "                   label=f'Median: {submission[\"predicted_risk_score\"].median():.2f}', linewidth=2)\n",
    "axes[0, 0].set_title('Predicted Risk Score Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Predicted Risk Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison with training distribution\n",
    "axes[0, 1].hist(train_risk['risk_score'], bins=50, alpha=0.5, label='Training Data', \n",
    "                color='steelblue', edgecolor='black')\n",
    "axes[0, 1].hist(submission['predicted_risk_score'], bins=50, alpha=0.5, label='Predictions', \n",
    "                color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Training vs Prediction Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Risk Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "box_data = [train_risk['risk_score'], submission['predicted_risk_score']]\n",
    "bp = axes[1, 0].boxplot(box_data, labels=['Training', 'Predictions'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "bp['boxes'][1].set_facecolor('coral')\n",
    "axes[1, 0].set_title('Risk Score Distribution: Training vs Predictions', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Risk Score')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Statistics comparison\n",
    "stats_comparison = pd.DataFrame({\n",
    "    'Training': [\n",
    "        train_risk['risk_score'].mean(),\n",
    "        train_risk['risk_score'].median(),\n",
    "        train_risk['risk_score'].std(),\n",
    "        train_risk['risk_score'].min(),\n",
    "        train_risk['risk_score'].max()\n",
    "    ],\n",
    "    'Predictions': [\n",
    "        submission['predicted_risk_score'].mean(),\n",
    "        submission['predicted_risk_score'].median(),\n",
    "        submission['predicted_risk_score'].std(),\n",
    "        submission['predicted_risk_score'].min(),\n",
    "        submission['predicted_risk_score'].max()\n",
    "    ]\n",
    "}, index=['Mean', 'Median', 'Std', 'Min', 'Max'])\n",
    "\n",
    "x = np.arange(len(stats_comparison.index))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, stats_comparison['Training'], width, label='Training', color='steelblue', alpha=0.7)\n",
    "axes[1, 1].bar(x + width/2, stats_comparison['Predictions'], width, label='Predictions', color='coral', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Statistic')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].set_title('Statistical Comparison', fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(stats_comparison.index)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / '11_final_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"  ✓ Saved: analysis_outputs/11_final_predictions.png\")\n",
    "\n",
    "# Save models\n",
    "for name, model in models_optimized.items():\n",
    "    if name != 'stacked':\n",
    "        with open(MODELS_DIR / f'{name}_advanced.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    else:\n",
    "        with open(MODELS_DIR / 'stacked_advanced.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "print(\"\\n✓ Models saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Model: {best_model_name.upper()}\")\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Predictions: {submission_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb72c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
